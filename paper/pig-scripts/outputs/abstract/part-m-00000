8	In this paper we consider a set of Software as a Service (SaaS) providers, that offer a set of Web services using the Cloud facilities provided by an Infrastructure as a Service (IaaS) provider. We assume that the IaaS provider offers a pay only what you use scheme similar to the Amazon EC2 service, comprising flat, on demand, and spot virtual machine instances. We propose a two-stage provisioning scheme. In the first stage, the SaaS providers determine the number of required flat and on demand instances by means of standard optimization techniques. In the second stage, the SaaS providers compete by bidding for the spot instances which are instantiated using the unused IaaS capacity. We put our focus on the bidding decision process by the SaaS providers, which takes place during the second stage, and apply N-armed bandit problems, in which the player is faced repeatedly with a choice among N different options, and every time he submits his decision evaluating past feedbacks. Through numerical experiments, we analyze proposed strategies under different scenarios and prove the SaaS providers ability to refine their behavior round by round and to determine the best bid so to maximize their revenue and achieve as many spot resources as possible, also addressing the importance of a trade-off between exploration and exploitation, i.e., among greedy and non-greedy actions.
36	In recent years, the rapid evolving Cloud Computing technologies multiply challenges including minimizing power consumption and meeting Quality-of- Services (QoS) requirements in the presence of heavy workloads from a large number of users using shared computing resources. Powering a middle-sized data center normally consumed 80,000kW power every year and computer servers consume around 5\% of the global power [1]. In order to address the skyrocket energy cost from the high level resource management aspect, we propose an energy efficient job scheduling approach based on a modified version of Weighted Round Robin scheduler that incorporates VMs reuse and live VM migration without compromising the Service Level Agreement (SLA). Enhanced Weighted Round Robin (EWRR) algorithm enhanced scheduler can monitor the running VMs status for possible VM consolidation or Migration. In addition, VM Manager observes the VMs utilization rate to start live migration from the over-utilizing Processing Element (PE) to under-utilized PEs or to the hibernated PEs by sending WOL (Wake-On-LAN) signal to activate them. Moreover, we have integrated our Dynamic Voltage and Frequency Scaling (DVFS) algorithm in CPU utilization model to specify the required frequency for each task depending on the task complexity and the deadline.
37	To provide elasticity for cloud-hosted applications, there is a need to specify the appropriate location of the resources (computing and storage resources) while meeting SLA promises. This paper presents literature review, problem statement and research methodology.
65	Energy consumption is one of the major expense in IT industry which has attracted the attentions of various data centers which deploys huge quantity of power supply. So attempt to reduce energy utilization is very prime concern in cloud computing as it supports various complex services obeying SLA. In this research work, we present an in-depth analysis of energy consumption at a core model of cloud computing in order to visualize the risk factors which is responsible for draining energy in data centers. We create an energy analysis which will consist of various cloudlet-IDs with estimation of SLA violation along with virtual machine migration with complete status capturing. Various energy host will be designed as real time virtual machine along with provisioning of various processing elements (PE) of datacenters using dynamic voltage frequency scaling methodology. The simulation will be designed in java environment to show the performance of our analysis approach to reduce energy consumption.
72	In hosting environments such as IaaS clouds, desirable application performance is usually guaranteed through the use of Service Level Agreements (SLAs), which specify minimal fractions of resource capacities that must be allocated for unencumbered use for proper operation. Arbitrary colocation of applications with different SLAs on a single host may result in inefficient utilization of the host's resources. In this paper, we propose that periodic resource allocation and consumption models -- often used to characterize real-time workloads -- be used for a more granular expression of SLAs. Our proposed SLA model has the salient feature that it exposes flexibilities that enable the infrastructure provider to safely transform SLAs from one form to another for the purpose of achieving more efficient colocation. Towards that goal, we present MORPHOSYS: a framework for a service that allows the manipulation of SLAs to enable efficient colocation of arbitrary workloads in a dynamic setting. We present results from extensive trace-driven simulations of colocated Video-on-Demand servers in a cloud setting. These results show that potentially-significant reduction in wasted resources (by as much as 60\%) are possible using MORPHOSYS.
87	In order to meet Service Level Agreement (SLA) requirements, Virtual Machine (VM) resources must be provisioned few minutes ahead due to the VM boot-up time. One way to do this is by predicting future resource demands. In this research, we have developed and evaluated cloud client prediction models for TPCW benchmark web application using three machine learning techniques: Support Vector Machine (SVM), Neural Networks (NN) and Linear Regression (LR). We included the SLA metrics for Response Time and Throughput to the prediction model with the aim of providing the client with a more robust scaling decision choice. Our results show that Support Vector Machine provides the best prediction model.
89	Resources dynamical allocation and management is always an important feature in cloud computing. Auto Scale allows users to scale their cloud resources capacity according to elastic loads timely, which has been widely used in mature public cloud. For private cloud, there are some different features from public cloud. It is more flexible to use Auto Scale technique to provide QoS guarantees and ensure system health. In this paper, we design a novel Auto Load-aware Scale scheme for private cloud environment. We describe scale in and scale out strategy based on prediction algorithm. We implement our scheme on OpenStack platform. Both simulation and experiments are carried out to evaluate our work. The experiments show that our scheme has better performance in resource utilization while providing high SLA levels.
96	"In resource provisioning for datacenters, an important issue is how resources may be allocated to an application such that the service level agreements (SLAs) are met. Resource provisioning is usually guided by intuitive or heuristic expectation of performance and existing user model. Provisioning based on such methodology, 
99	We consider several reliability problems that arise when allocating applications to processing resources in a Cloud computing platform. More specifically, we assume on the one hand that each computing resource is associated to a capacity constraint and to a probability of failure. On the other hand, we assume that each service runs as a set of independent instances of identical Virtual Machines, and that the Service Level Agreement between the Cloud provider and the client states that a minimal number of instances of the service should run with a given probability. In this context, given the capacity and failure probabilities of the machines, and the capacity and reliability demands of the services, the question for the cloud provider is to find an allocation of the instances of the services (possibly using replication) onto machines satisfying all types of constraints during a given time period. In this paper, our goal is to assess the impact of the reliability constraint on the complexity of resource allocation problems. We consider several variants of this problem, depending on the number of services and whether their reliability demand is individual or global. We prove several fundamental complexity results (\#P' and NP-completeness results) and we provide several optimal and approximation algorithms. In particular, we prove that a basic randomized allocation algorithm, that is easy to implement, provides optimal or quasi-optimal results in several contexts, and we show through simulations that it also achieves very good results in more general settings.
103	We consider robust resource allocation of services in Clouds. More specifically, we consider the case of a large public or private Cloud platform that runs a relatively small set of large and independent services. These services are characterized by their demand along several dimensions (CPU, memory,...) and by their quality of service requirements, that have been defined through an SLA in the case of a public Cloud or fixed by the administrator in the case of a private Cloud. This quality of service defines the required robustness of the service, by setting an upper limit on the probability that the provider fails to allocate the required quantity of resources. This maximum probability of failure can be transparently turned into a pair (price, penalty). Failures can indeed hit the platform, and resilience is provided through service replication. Our contribution is two-fold. Firstly, we propose a resource allocation strategy whose complexity is logarithmic in the number of resources, what makes it very efficient for large platforms. Secondly, we propose an efficient algorithm based on rare events detection techniques in order to estimate the robustness of an allocation, a problem that has been proven to be #P-complete. Finally, we provide an analysis of the proposed strategy through an extensive set of simulations, both in terms of the overall number of allocated resources and in terms of time necessary to compute the allocation.
115	"Energy-related costs have become one of the major economic factors in IT data-centers, and companies and the research community are currently working on new efficient power-aware resource management strategies, also known as ""Green IT"". Here we propose a framework for autonomic scheduling of tasks and web-services on cloud environments, optimizing the profit taking into account revenue for task execution minus penalties for service-level agreement violations, minus power consumption cost. The principal contribution is the combination of consolidation and virtualization technologies, mathematical optimization methods, and machine learning techniques. The data-center infrastructure, tasks to execute, and desired profit are casted as a mathematical programming model, which can then be solved in different ways to find good task scheduling. We use an exact solver based on mixed linear programming as a proof of concept but, since it is an NP-complete problem, we show that approximate solvers provide valid alternatives for finding approximately optimal schedules. The machine learning is used to estimate the initially unknown parameters of the mathematical model. In particular, we need to predict a priori resource usage (such as CPU consumption) by different tasks under current workloads, and estimate task service-level-agreement (such as response time) given workload features, host characteristics, and contention among tasks in the same host. Experiments show that machine learning algorithms can predict system behavior with acceptable accuracy, and that their combination with the exact or approximate schedulers manages to allocate tasks to hosts striking a balance between revenue for executed tasks, quality of service, and power consumption."
120	Resource scheduling algorithm for ForCES (Forwarding and Control Element Separation) networks need to meet the flexibility, programmability and scalability of node resources. DBC (Deadline Budget Constrain) algorithm relies on users select cost or time priority, then scheduling to meet the requirements of users. However, this priority strategy of users is relatively simple, and cannot adapt to dynamic change of resources, it is inevitable to reduce the QoS. In order to improve QoS, we refer to the economic model and resource scheduling model of cloud computing, use SAL (Service Level Agreement) as pricing strategy, on the basis of DBC algorithm, propose an DABP (Deadline And Budget Priority based on DBC) algorithm for ForCES networks, DABP combines both budget and time priority to scheduling. In simulation and test, we compare the task finish time and cost of DABP algorithm with DP (Deadline Priority) algorithm and BP (Budget Priority) algorithm, the analysis results show that DABP algorithm make the task complete with less cost within deadline, benifical to load balancing of ForCES networks.
133	Cloud computing has been established in recent years as an important area of research. This reality has been consolidated because, currently, tasks such as obtainment, manipulation, sharing and exploitation of large amounts of data are common and require many computational resources, so cloud computing can contribute because it can provide this resources indefinitely, including processing, memory, storage and others, all them for immediate use. Cloud computing has many challenges, among them, the specification of work platforms in a cloud environment. In this context, the use of models can help insofar as they may contain several types of information that can be handled with the intention of automating the process of creation of an environment. A template can contain information about the user, hardware and software that can be used by computer systems to automatically build the necessary infrastructure for the operation of a virtual machine on a cloud computing environment. In addition, models can be easily exported to other formats used by cloud services providers. The use of concepts of Service Level Agreements - SLA to control the utilization of computational resources of a provider is common in computing environments and can be used to ensure the quality of services that being offered. The objective of this work is to combine these paradigms, cloud, models and SLA, in order to use the resources of a computational cloud to provide automatically a platform as a service - PaaS. This project will be developed in a web environment, using a model-driven approach to create the platforms and the management of resources and quality will be made by SLA.
152	Current trends in virtualization, green computing, and cloud computing require ever increasing efficiency in consolidating virtual machines without degrading quality of service. In this work, we consider consolidating virtual machines on the minimum number of physical containers (e.g., hosts or racks) in a cloud where the physical network (e.g., network interface or top of the rack switch link) may become a bottleneck. Since virtual machines do not simultaneously use maximum of their nominal bandwidth, the capacity of the physical container can be multiplexed. We assume that each virtual machine has a probabilistic guarantee on realizing its bandwidth Requirements-as derived from its Service Level Agreement with the cloud provider. Therefore, the problem of consolidating virtual machines on the minimum number of physical containers, while preserving these bandwidth allocation guarantees, can be modeled as a Stochastic Bin Packing (SBP) problem, where each virtual machine's bandwidth demand is treated as a random variable. We consider both offline and online versions of SBP. Under the assumption that the virtual machines' bandwidth consumption obeys normal distribution, we show a 2-approximation algorithm for the offline version and improve the previously reported results by presenting a (2 +_)-competitive algorithm for the online version. We also observe that a dual polynomial-time approximation scheme (PTAS) for SBP can be obtained via reduction to the two-dimensional vector bin packing problem. Finally, we perform a thorough performance evaluation study using both synthetic and real data to evaluate the behavior of our proposed algorithms, showing their practical applicability.
165	Grid computing, one of the latest buzzwords in the ICT industry, is emerging as a new paradigm for Internet-based parallel and distributing computing. Despite a number of advances in grid computing, resource management and application scheduling in such environments continues to be a challenging and complex undertaking. This is due to geographic distribution of grid resources owned by different organizations with different usage policies, cost models and varying load and availability patterns with time. This tutorial introduces fundamental principles of grid computing and computational economy and discusses how they impact on emerging computational and data grid technologies. It identifies resource management challenges and introduces new challenges and requirements introduced by the grid economy on grid service providers (GSPs) and grid service consumers. The tutorial presents a service-oriented grid architecture inspired by computational economies and demonstrates how it can be realized by leveraging the existing grid technologies and building new economic-oriented capabilities and components. We present solutions to these challenges based on our experience in designing and developing market-oriented Gridbus technologies such as Grid Market Directory, Grid Bank, Grid Service Broker, Workflow Engine, and SLA-based enterprise Grid Resource Allocation system. Case studies on the use of Gridbus middleware in the creation of various e-science and e-business applications and their deployment on national/international utility-oriented grids along with its impact on emerging cloud computing paradigm will also be highlighted.
166	As Clouds are complex, large-scale, and heterogeneous distributed systems, management of their resources is a challenging task. They need automated and integrated intelligent strategies for provisioning of resources to offer services that are secure, reliable, and cost-efficient. Hence, effective management of services becomes fundamental in software platforms that constitute the fabric of computing Clouds. In this direction, this paper identifies open issues in autonomic resource provisioning and presents innovative management techniques for supporting SaaS applications hosted on Clouds. We present a conceptual architecture and early results evidencing the benefits of autonomic management of Clouds.
168	Cloud computing systems promise to offer subscription-oriented, enterprise-quality computing services to users worldwide. With the increased demand for delivering services to a large number of users, they need to offer differentiated services to users and meet their quality expectations. Existing resource management systems in data centers are yet to support Service Level Agreement (SLA)-oriented resource allocation, and thus need to be enhanced to realize cloud computing and utility computing. In addition, no work has been done to collectively incorporate customer-driven service management, computational risk management, and autonomic resource management into a market-based resource management system to target the rapidly changing enterprise requirements of Cloud computing. This paper presents vision, challenges, and architectural elements of SLA-oriented resource management. The proposed architecture supports integration of market-based provisioning policies and virtualisation technologies for flexible allocation of resources to applications. The performance results obtained from our working prototype system shows the feasibility and effectiveness of SLA-based resource provisioning in Clouds.
170	"Welcome to the special issue of Future Generation Computer System (FGCS) journal. This special issue compiles a number of excellent technical contributions that significantly advance the state-of-the-art in federated management [2][3][4] of Grid and Cloud computing environments. Federated management of administratively distributed grids and clouds offers significant benefits including: (i) improving the ability of resource providers to meet SLA compliance [5] for clients and offer improved service by optimizing the service placement and throughput according to users' QoS needs
172	With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, we define Cloud computing and provide the architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as Virtual Machines (VMs). We also provide insights on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain Service Level Agreement (SLA)-oriented resource allocation. In addition, we reveal our early thoughts on interconnecting Clouds for dynamically creating global Cloud exchanges and markets. Then, we present some representative Cloud platforms, especially those developed in industries, along with our current work towards realizing market-oriented resource allocation of Clouds as realized in Aneka enterprise Cloud technology. Furthermore, we highlight the difference between High Performance Computing (HPC) workload and Internet-based services workload. We also describe a meta-negotiation infrastructure to establish global Cloud exchanges and markets, and illustrate a case study of harnessing �Storage Clouds� for high performance content delivery. Finally, we conclude with the need for convergence of competing IT paradigms to deliver our 21st century vision.
180	With the large-scale deployment of virtualized data centers, energy consumption and SLA (Service Level Agreement) violation have already become the urgent issue to be solved. And it is essential and important to design energy-aware allocation policy for energy-aware and SLA violation reduction. In this paper, we propose a novel allocation and selection policy for the dynamic virtual machine (VM) consolidation in virtualized data centers to reduce energy consumption and SLA violation. Firstly, we use the mean and standard deviation of CPU utilization for VM to determine the hosts overloaded or not, secondly we use the positive maximum correlation coefficient to select VMs from those overloading hosts for migration. Although the proposed allocation and selection policies performs a little worse than the previous ones in energy consumption, experiments show that it performs greatly better than the previous ones on the whole.
181	In the today Internet of Services, one of the challenges of Application Service Providers (ASPs) is to fulfill the QoS requirements stated in the Service Level Agreements (SLAs) established with different consumers and to minimize the investment and management costs. Cloud computing is the promising solution for ASPs that increasingly demand for an elastic infrastructure. In this paper, we formulate the ASP resource management as an optimization problem and propose both reactive and proactive heuristic policies that approximate the optimal solution. The proposed policies leverage on information about the system performance history and can be applied at runtime because of their reduced computational time. Our experimental results show that some heuristics based on prediction approximate the exact knowledge of the workload.
186	We present a probabilistic and a possibilistic model for assessing the risk of a service level agreement for a computing task in a cluster/grid environment. These models can also be applied to cloud computing. Using the predictive probabilistic approach we develop a framework for resource management in grid computing, and by introducing an upper limit for the number of failures we approximate the probability that a particular computing task is successful. In the predictive possibility model we estimate the possibility distribution of the future number of node failures by a fuzzy nonparametric regression technique. Then the resource provider can use the probabilistic or the possibilistic model to get alternative risk assessments.
188	Cloud computing offers on-demand access to computational resources. One of the major challenges in cloud environments is to enforce the elasticity of the processes that execute in the cloud, avoiding Service Level Agreements (SLAs) violations and reducing waste with idle resources. We propose an autonomic resource management system for cloud computing, called VOLTAIC (Volume Optimization Layer To AssIgn Cloud resources). The proposal analyzes usage profiles of physical and virtual elements and defines heuristics based on differential utilization level that guarantee an enhanced allocation of virtual elements. VOLTAIC introduces algorithms to determine proper parameters to allocate cloud elements and to automatically migrate those elements to avoid performance degradation due to server saturation. Results obtained through the implementation of the system in a small-scale cluster show that the system efficiently assigns virtual elements and ensures proper resource allocation to virtual elements. We also developed a virtual network simulator for cloud environments to attest the high performance of VOLTAIC in broader scenarios. Results show improvements in up to 10% in the amount of offered cycles due to correct assignment of virtual elements.
189	Cloud computing offers on-demand access to computational resources. One of the major challenges in cloud environments is to enforce the elasticity of the processes that execute in the cloud, avoiding Service Level Agreements (SLAs) violations and reducing waste with idle resources. We propose an autonomic resource management system for cloud computing, called VOLTAIC (Volume Optimization Layer To AssIgn Cloud resources). The proposal analyzes usage profiles of physical and virtual elements and defines heuristics based on differential utilization level that guarantee an optimized allocation of virtual elements. VOLTAIC introduces algorithms to determine proper parameters to allocate cloud elements and to automatically migrate those elements to avoid performance degradation due to server saturation. Results obtained through the implementation of the system in a small-scale cluster show that the system efficiently assigns virtual elements and ensures proper resource allocation to virtual elements. We also developed a virtual network simulator for cloud environments to attest the applicability of VOLTAIC in broader scenarios. Results show reductions in up to 10 % in the amount of idle cycles due to correct assignment of virtual elements.
190	The elasticity afforded by cloud computing allows consumers to dynamically request and relinquish computing and storage resources and pay for them on a pay-per-use basis. Cloud computing providers rely on virtualization techniques to manage the dynamic nature of their infrastructure allowing consumers to dynamically allocate and deallocate virtual machines of different capacities. Cloud providers need to optimally decide the best allocation of virtual machines to physical machines as the demand varies dynamically. When making such decisions, cloud providers can migrate VMs already allocated and/or use external cloud providers. This paper considers the problem in which the cloud provider wants to maximize its revenue, subject to capacity, availability SLA, and VM migration constraints. The paper presents a heuristic solution, called Near Optimal (NOPT), to this NP-hard problem and discusses the results of its experimental evaluation in comparison with a best fit (BF) allocation strategy. The results show that NOPT provides a 45% improvement in average revenue when compared with BF for the parameters used in the experiment. Moreover, the NOPT algorithm maintained the availability close to one for all classes of users while BF exhibited a lower availability and even failed to meet the availability SLA at times.
191	A challenge in cloud resource management is to design self-adaptable solutions capable to react to unpredictable workload fluctuations and changing utility principles. This paper analyzes the problem from the perspective of an Application Service Provider (ASP) that uses a cloud infrastructure to achieve scalable provisioning of its services in the respect of QoS constraints. First we draw a taxonomy of IaaS provider and use the identified features to drive the design of four autonomic service management architectures differing on the degree of control an ASP have on the system. We implemented two of this solutions and related mechanism to test five different resource provisioning policies. The implemented testbed has been evaluated under a realistic workload based on Wikipedia access traces on Amazon EC2 platform. The experimental evaluation performed confirms that: the proposed policies are capable to properly dimension the system resources making the whole system self-adaptable respect to the workload fluctuation. Moreover, having full control over the resource management plan allow to save up to the 32\% of resource allocation cost always in the respect of SLA constraints.
193	"The complexity of cloud systems poses new infrastructure and application management challenges. One of the common goals of the research community, practitioners and vendors is to design self-adaptable solutions capable to react to unpredictable workload fluctuations and changing utility principles. This paper analyzes the problem from the perspective of an application service provider that uses a cloud infrastructure to achieve scalable provisioning of its services in the respect of QoS constraints. In the specific, we propose four architectural schemas for autonomic resource management of cloud-based systems. The proposed solutions, differing for the degree of control on the autonomic cycle phases, have been designed considering: (1) functional requirements dictated by the resource provisioning solutions proposed in literature
200	Efficient resource provisioning and allocation enable the reduction of energy consumption in cloud providers and make providers to get more profits. When the client submits more and more jobs to a cloud provider, the available resources in a cloud provider may not be enough for completing these jobs. Therefore, the cloud providers have to lease more available resources from others. In order to solve this problem, this study proposes a combinatorial auction-based approach for dynamic resource provisioning and allocation, considering the client requests with deadline. Experimental results show that our proposed auction-based approach could get more profit.
206	The recent cloud computing paradigm represents a trend of moving business applications to platforms run by parties located in different administrative domains. A cloud platform is often highly scalable and cost-effective through its pay-as-you-go pricing model. However, being shared by a large number of users, the running of applications in the platform faces higher performance uncertainty compared to a dedicated platform. Existing Service Level Agreements (SLAs) cannot sufficiently address the performance variation issue. In this paper, we use utility theory leveraged from economics and develop a new utility model for measuring customer satisfaction in the cloud. Based on the utility model, we design a mechanism to support utility-based SLAs in order to balance the performance of applications and the cost of running them. We consider an infrastructure-as-a-service type cloud platform (e.g., Amazon EC2), where a business service provider leases virtual machine (VM) instances with spot prices from the cloud and gains revenue by serving its customers. Particularly, we investigate the interaction of service profit and customer satisfaction. In addition, we present two scheduling algorithms that can effectively bid for different types of VM instances to make tradeoffs between profit and customer satisfaction. We conduct extensive simulations based on the performance data of different types of Amazon EC2 instances and their price history. Our experimental results demonstrate that the algorithms perform well across the metrics of profit, customer satisfaction and instance utilization.
207	In cloud datacenters, effective resource provisioning is needed to maximize energy efficiency and utilization of cloud resources while guaranteeing the Service Level Agreement (SLA) for tenants. Previous resource provisioning strategies either allocate physical resources to virtual machines (VMs) based on static VM resource demands or dynamically handle the variations in VM resource requirements through live VM migrations. However, the former fail to maximize energy efficiency and resource utilization while the latter produce high migration overhead. To handle these problems, we propose an initial VM allocation mechanism that consolidates complementary VMs with spatial/temporal-awareness. Complementary VMs are the VMs whose total demand of each resource dimension (in the spatial space) nearly reaches their host's capacity during VM lifetime period (in the temporal space). Based on our observation of the existence of VM resource utilization patterns, the mechanism predicts the lifetime resource utilization patterns of short-term VMs or periodical resource utilization patterns of long-term VMs. Based on the predicted patterns, it coordinates the requirements of different resources and consolidates complementary VMs in the same physical machine (PM). This mechanism reduces the number of PMs needed to provide VM service hence increases energy efficiency and resource utilization and also reduces the number of VM migrations and SLA violations. Simulation based on two real traces and real-world testbed experiments show that our initial VM allocation mechanism significantly reduces the number of PMs used, SLA violations and VM migrations of the previous resource provisioning strategies.
208	A number of challenges in implementing cloud technique related to further improving Web application performance and decreasing the cost. In order to achieve high profits, cloud-based web application providers must carefully balance cloud resources and dynamic workloads. However, this task is usually difficulty because of the complex nature of most web application. In this paper, we presented a predictive performance model to analyze such applications and to determine when and how much resource to allocate to each tier of an application. In addition, we proposed a new profit model to describe revenues specified by the Service Level Agreement (SLA) and costs generated by leased resources. Furthermore, we employed profit driven model to guide our resource management algorithms to maximize the profits earned to the service providers. We also designed and implemented a simulation experiment on CloudSim that adopts our proposed methodology. Experimental results indicated that our model faithfully captures the performance and resources are allocated properly in response to the changing workload, thus the goal of maximizing the profit has been achieved.
220	Higher hardware utilization and SLA fulfillment are two main objectives of any infrastructure cloud, thus, making resource allocation to virtual machines as one of the most critical aspect. Haizea is a popular cloud lease manager which supports a variety of resource leases according to the application requirements. These leases are: Immediate, Best-Effort, Deadline Sensitive and Advance Reservation (AR). But all these leases are static in nature viz. once resources are allocated to a lease, these resources cannot be altered during the complete lifetime of the lease. This contradicts with the philosophy and implementation of a pure on-demand elastic cloud where resources of a lease are continuously monitored against the utilization and altered based on the requirements. Our work aims at the inclusion of three important features to mitigate this shortcoming in Haizea. First, it introduces a new class of lease: Dynamic lease to accommodate resource changes. Second, it examines virtual machine resource utilization to decide about the demand and need of allocation change and third, it accommodates the expected changes in resource allocation by introducing two new sub-leases which allow dynamic resource allocation in the schedule. Lease experiments are performed on Haizea to validate the introduced features.
228	Virtualization provides an efficient solution to the objectives of the cloud computing paradigm by facilitating creation of Virtual Machines (VMs) over the underlying physical servers, leading to improved resource utilization and intellection. Virtualization proposes to create a virtual version of a device or a resource likely to virtualize a server, a storage space, operating system or even network where the mechanism divides the resource into one or more execution environments. To analyze the behavior of a Cloud data center by means of various aspects like availability, utilization, responsiveness and waiting time. The factors that a cloud provider must take into account are elasticity, scalability, live migration of VMs and performance isolation. Live migration of VMs, the process of dynamically transferring a virtual machine across different servers, hasproved to represent a new opportunity to enable responsive and dynamic proved torepresent a new opportunity toenable responsive and dynamic resource management in modern data centers. Migrations of VMs is performed on the basis of overloading that occurs in physical servers in cloud data centers, whichleads to relatively less performance degradation. The SLA violation is also less compared to other techniques and this means the cloud provider will incur less cost from VM migrations. Further, MHOD algorithm performs host selection and VM reallocation quicker than the existing algorithms. Maintenance of physical servers can be efficiently achieved through our algorithm as it leads to efficient consolidation of VMs.
229	It is clear that Cloud computing is and will be a sea change for the Information Technology by changing the way in which both software and hardware are designed and purchased. In this work we address the use of this emerging computing paradigm into web hosting providers in order to avoid its resource management limitations. Thanks to the Cloud approach, resources can be provided in a dynamic way according with the needs of providers and end-users. In this paper, we present an elastic web hosting provider, namely Cloud Hosting Provider (CHP), that makes use of the outsourcing technique in order to take advantage of Cloud computing infrastructures for providing scalability and high availability capabilities to the web applications deployed on it. Furthermore, we pursue the main goal of maximizing the revenue earned by the provider through both the analysis of Service Level Agreements (SLA) and the employment of an economic model. The evaluation exposed demonstrates that the system proposed is able to properly react to the dynamic load received by the web applications and it also achieve the aforesaid revenue maximization of the provider by performing an SLA-aware resource (i.e. web servers) management.
239	Cloud computing has attracted increasing attention in recent years. With the growth in the number and frequency of applications being deployed into clouds, the burden of resource management of cloud providers is becoming heavier. The resource deployment must satisfy the need about the performance, availability and reliability of applications from the view of clients, but also ensure the high resource utilization of the cloud providers. In this paper, we design a multi-objective serial optimization with priorities approach, named RMORM, to find the resource deployment in clouds rapidly. This approach is of great practical significance and engineering value and scalable to add new constraints.
240	Network bandwidth is a key resource in cloud computing. Its inefficient allocation still poses issues in a virtualized cloud environment. Efficient and dynamic resource allocation is essential to fulfill Quality of Service (QoS). In this paper, we propose an optimized dynamic bandwidth allocation strategy based on SLA and previous traffic statistics. The algorithm uses previous traffic statistics to predict future bandwidth requirement for every virtual machine. Optimized Bandwidth Manager (OBM) allocates this predicted bandwidth to virtual machine by applying readjusted bandwidth limits directly to the Virtual Network Interfaces.
244	Cloud Computing has become the most popular distributed computing environment because it does not require any user level management and controlling on the low-level implementation of the system. However, efficient resource provisioning is a key challenge for cloud computing and resolving such kind of problem can reduce under or over utilization of resources, increase user satisfaction by serving more users during peak hours, reduce implementation cost for providers and service cost for users. Existing works on cloud computing focuses to accurate estimation of the capacity needs, static or dynamic VM (Virtual Machine) creation and scheduling. But significant amount of time is required to create and destroy VMs which could be used to serve more user requests. In this paper, an adaptive QoS (Quality of Service) aware VM provisioning mechanism is developed that ensures efficient utilization of the system resources. The VM for similar type of requests has been recycled so that the VM creation time can be minimized and used to serve more user requests. In the proposed model, QoS is ensured by serving all the tasks within the requirements described in SLA. Tasks are separated using multilevel queue and the most urgent task is given high priority. The simulation-based experimental results shows that a great number of tasks can be served compared to others which will help to satisfy customers during the peak hour.
255	"Cloud computing is receiving increasingly attention as it provides infinite resource capacity and ""pay-as-you-go"" resource usage pattern to hosted applications. To maintain its SLA targets, resource provisioning of service-oriented applications in the cloud requires reliable performance from the cloud resources. In this paper, we study performance behavior of small instances in Amazon EC2. We demonstrate that the performance of virtual instances is relatively stable over time with fluctuations of mean response time within at most 8% of the longterm average. Moreover, we also show that different supposedly identical instances often have very different performance, up to a ratio 4 from each other. We consider this as an important issue that must be addressed, but also as an opportunity as it allows one to assign each instance with a task that matches its own performance profile."
259	Elasticity in cloud systems provides the flexibility to acquire and relinquish computing resources on demand. However, in current virtualized systems resource allocation is mostly static. Resources are allocated during VM instantiation and any change in workload leading to significant increase or decrease in resources is handled by VM migration. Hence, cloud users tend to characterize their workloads at a coarse grained level which potentially leads to under-utilized VM resources or under performing application. A more flexible and adaptive resource allocation mechanism would benefit variable workloads, such as those characterized by web servers. In this paper, we present an elastic resources framework for IaaS cloud layer that addresses this need. The framework provisions for application workload forecasting engine, that predicts at run-time the expected demand, which is input to the resource manager to modulate resource allocation based on the predicted demand. Based on the prediction errors, resources can be over-allocated or under-allocated as compared to the actual demand made by the application. Over-allocation leads to unused resources and under allocation could cause under performance. To strike a good trade-off between over-allocation and under-performance we derive an excess cost model. In this model excess resources allocated are captured as over-allocation cost and under-allocation is captured as a penalty cost for violating application service level agreement (SLA). Confidence interval for predicted workload is used to minimize this excess cost with minimal effect on SLA violations. An example case-study for an academic institute web server workload is presented. Using the confidence interval to minimize excess cost, we achieve significant reduction in resource allocation requirement while restricting application SLA violations to below 2-3\%.
265	Cloud is strongly emerging as the new deal of distributed computing. One of the reason behind the Cloud success is its business/commercial-oriented nature, proof of its effectiveness and applicability to real problems. There are actually a lot of open-private Cloud infrastructures aiming at providing dynamic-on demand resource provisioning according to the IAAS paradigm. To this purpose they usually apply a best effort policy, without taking into account service level agreements (SLA) and related quality of service (QoS) requirements. In such a context, the main goal of Cloud@Home is to implement a volunteer-Cloud paradigm which allows to aggregate Cloud infrastructure providers. In this work we specifically focus on SLA-QoS aspects, describing how to provide SLA based QoS guarantees through Cloud@Home on top of non-QoS oriented Cloud Providers. Aim of the paper is to demonstrate how Cloud@Home can fulfil such goal, providing and specifying the architecture, the algorithms and the components that implement SLA-QoS management features.
268	The popularity and commercial use of cloud computing has prompted an increased concern among cloud service providers for energy efficiency while still maintaining quality of service. One of the key techniques used for the efficient use of cloud server resources is virtual machine placement. This work introduces a precise VM placement algorithm that ensures energy efficiency and also prevents Service Level Agreement (SLA) violation. The mathematical model of the algorithm is supported by a sophisticated data analytic system implemented as a service. The precision of the algorithm is achieved by allowing each individual VM to build its own data model on demand over an appropriate time horizon. Thus the data model can reflect accurately the characteristics of resource usage of the VM. The algorithm can communicate synchronously or asynchronously with the data analytic service which is deployed as a cloud-based solution. In the experiments, several advanced data modelling and use forecasting techniques were evaluated. Results from simulation-based experiments show that the VM placement algorithm (supported by the data analytic service) can effectively reduce power consumption, the number of VM migrations, and prevent SLA violation, it also compares very favourably with other placement algorithms.
272	Data centres are powerful ICT facilities which constantly evolve in size, complexity, and power consumption. At the same time users' and operators' requirements become more and more complex. However, existing data centre frameworks do not typically take energy consumption into account as a key parameter of the data centre's configuration. To lower the power consumption while fulfilling performance requirements we propose a flexible and energy-aware framework for the (re)allocation of virtual machines in a data centre. The framework, being independent from the data centre management system, computes and enacts the best possible placement of virtual machines based on constraints expressed through service level agreements. The framework's flexibility is achieved by decoupling the expressed constraints from the algorithms using the Constraint Programming (CP) paradigm and programming language, basing ourselves on a cluster management library called Entropy. Finally, the experimental and simulation results demonstrate the effectiveness of this approach in achieving the pursued energy optimization goals.
278	In this paper we consider the general problem of resource provisioning within cloud computing. We analyze the problem of how to allocate resources to different clients such that the service level agreements (SLAs) for all of these clients are met. A model with multiple service request classes generated by different clients is proposed to evaluate the performance of a cloud computing center when multiple SLAs are negotiated between the service provider and its customers. For each class, the SLA is specified by the request rejection probabilities of the clients in that class. The proposed solution supports cloud service providers in the decision making about 1) defining realistic SLAs, 2) the dimensioning of data centers, 3) whether to accept new clients, and 4) the amount of resources to be reserved for high priority clients. We illustrate the potential of the solution by a number of experiments conducted for a large and therefore realistic number of resources.
281	In recent years the advances in cloud computing and virtualization have created the need for autonomic resource provision. The correct provisioning of resources is a difficult task due to variations and uncertainty in workload demands. Most data center workload demands are very spiky in nature and often vary significantly during the course of a single day. Because the resource availability in a data center is generally unpredictable due to the shared feature of the cloud resources and because of the stochastic nature of the workload, severe service level agreement (SLA) violations may occur frequently. To overcome this problem, researcher's attention is diverted towards developing dynamic resource management strategies. In this paper, an autonomic resource controller is proposed that dynamically controls the resource allocation for data center's virtual containers. The controller has two parts: A resource modeler that models the non-linearity of the system by employing different Machine Learning techniques allowing the datacenter to allocate the appropriate resources and a resource fuzzy tuner that dynamically tunes the allocated resources using fuzzy logic to sustain the desired performance taking into consideration the enforcing of service differentiation among clients. Experimental results on a real data center dataset showed that the proposed resource controller can predict future resource needs while still sustaining performance goals stated in the SLA. Also, using the bagging and the boosting techniques along with model tree classifiers was demonstrated to be promising in terms of accuracy and performance.
290	"The necessity and significance of improving the energy efficiency of cloud implementations have increased due to the rapid growth and proliferation of cloud computing services around the world. Virtual machines (VMs) comprise the backend of most, if not all, cloud computing services. Several VMs are often consolidated on a physical machine to efficiently utilize its resources. In this paper, we take into account the cooling and network structure of the datacenter host ing the physical machines when consolidating the VMs so that fewer racks and routers are employed, without compromising the service-level agreements
291	In this paper, we study a resource allocation problem in the context of Cloud Computing, in which a set of Virtual Machines (VM) has to be allocated on a set of Physical Machines (PM). Each VM has a given demand (e.g. CPU demand), and each PM has a capacity. However, VMs only use a fraction of their demand. The aim is to exploit the difference between the demand of the VM and its actual resource usage, to achieve a higher utilization on the PMs. However, the resource consumption of the VMs might change over time (while staying under its original demand), implying sometimes expensive �SLA violations � when the demand of some VMs is not satisfied because of overloaded PMs. Thus, while optimizing the global resource utilization of the PMs, it is necessary to ensure that at any moment a VM�s need evolves, a few number of migrations (moving a VM from PM to PM) is sufficient to find a new configuration in which all the VMs � consumptions are satisfied. We model this problem using a fully dynamic bin packing approach and we present an algorithm ensuring a global utilization of the resources of 66%. Moreover, each time a PM is overloaded, at most one migration is sufficient to fall back in a configuration with no overloaded PM, and at most 3 different PMs are concerned by required migrations that may occur to keep the global resource utilization correct. This allows the platform to be highly resilient to a great number of changes.
292	In this paper, we study a resource allocation problem in the context of Cloud Computing, in which a set of Virtual Machines (VM) has to be allocated on a set of Physical Machines (PM). Each VM has a given demand (e.g. CPU demand), and each PM has a capacity. However, VMs only use a fraction of their demand. The aim is to exploit the difference between the demand of the VM and its actual resource usage, to achieve a higher utilization on the PMs. However, the resource consumption of the VMs might change over time (while staying under its original demand), implying sometimes expensive �SLA violations� when the demand of some VMs is not satisfied because of overloaded PMs. Thus, while optimizing the global resource utilization of the PMs, it is necessary to ensure that at any moment a VM's need evolves, a few number of migrations (moving a VM from PM to PM) is sufficient to find a new configuration in which all the VMs' consumptions are satisfied. We model this problem using a fully dynamic bin packing approach and we present an algorithm ensuring a global utilization of the resources of 66\%. Moreover, each time a PM is overloaded, at most one migration is sufficient to fall back in a configuration with no overloaded PM, and at most 3 different PMs are concerned by required migrations that may occur to keep the global resource utilization correct. This allows the platform to be highly resilient to a great number of changes.
297	Recently, the development of cloud computing has received considerable attention. For cloud service providers, packing VMs onto a small number of servers is an effective way to reduce energy costs, so as to improve the efficiency of the data center. However allocating too many VMs on a physical machine may cause some hot spots which violate the SLA of applications. Load balancing of the entire system is hence needed to guarantee the SLA. In this paper, we present a simulated-annealing load balancing algorithm for solving the resource allocation and scheduling problem in a cloud computing environment. Experimental results show that this method is able to achieve load balancing, and performs better than the round robin and basic simulated-annealing algorithms.
298	Cloud data centers and virtualization are being highly considered for enterprises and industries. However, elastic fine-grained resource provision while ensuring performance and SLA guarantees for applications requires careful consideration of important and extremely challenging tradeoffs. In this paper, we present RPPS (Cloud Resource Prediction and Provisioning scheme), a scheme that automatically predict future demand and perform proactive resource provisioning for cloud applications. RPPS employs the ARIMA model to predict the workloads in the future, combines both coarse-grained and fine-grained resource scaling under different situations, and adopts a VM-complementary migration strategy. RPPS can resolve predictive resource provisioning problem when enterprises confront demand fluctuations in cloud data center. We evaluate a prototype of RPPS with traces collected by ourselves using typical CPU intensive applications and as well as workloads from a real data center. The results show that it not only has high prediction accuracy (about 90\% match in most time) but also scales the resource well.
299	Many of the problems facing providers and users of Cloud-based systems, in terms of maximizing Quality-of-Service (QoS) satisfaction, can be studied using theories in microeconomics. Specifically, the concept of market-based control provides tools that can be used to design economically and computationally efficient Cloud software architectures. This chapter surveys both domains and presents some of the underlying problems and opportunities in the interesting crossbreed of economics and Cloud computing. The dynamic resource allocation problem is used as an example to demonstrate the added value of this approach. Observations from simulation studies reveal the usefulness of the posted offer market model as a viable mechanism for orchestrating the interaction of components in a Cloud software architecture. The chapter concludes with a reflection on open problems that need to be addressed to move the area forward.
307	Compared with the traditional computing models such as grid computing and cluster computing, a key advantage of Cloud computing is that it provides a practical business model for customers to use remote resources. However, it is challenging for Cloud providers to allocate the pooled computing resources dynamically among the differentiated customers so as to maximize their revenue. It is not an easy task to transform the customer-oriented service metrics into operating level metrics, and control the Cloud resources adaptively based on Service Level Agreement (SLA). This paper addresses the problem of maximizing the provider's revenue through SLA-based dynamic resource allocation as SLA plays a vital role in Cloud computing to bridge service providers and customers. We formalize the resource allocation problem using Queuing Theory and propose optimal solutions for the problem considering various Quality of Service (QoS) parameters such as pricing mechanisms, arrival rates, service rates and available resources. The experimental results, both with the synthetic dataset and with traced dadataset, show that our algorithms outperform related work.
316	Applications are shifting into large scale, virtualized data centres that provide resources on a pay-per-usage basis. Data centres must minimize resource consumption while providing enough resources to meet application requirements. To meet highly variable application demands, a dynamic approach to virtual machine (VM) management is required. This involves three basic management operations: (i) placing newly arrived VMs, (ii) migrating (moving) VMs off of highly utilized machines to avoid performance degradation, and (iii) migrating VMs off of underutilized machines so that they may be shut down to save power. We define a management strategy to consist of a set of policies that guide these three operations. We consider the goals of minimizing Service Level Agreement violations and minimizing power consumption. Developing a management strategy to achieve both of these goals is challenging, as the goals are often in conflict. We propose achieving both goals through dynamically switching between two management strategies, each with a single goal, depending on current data centre state. We propose three methods of dynamically switching strategies, and evaluate these methods through simulation. Dynamic strategy switching offers improved results over a single management strategy.
326	Cloud Computing is an emerged technology of the domain High Performance Computing and serves the community for getting their services executed over the internet. Cloud Computing has become the most popular distributed computing environment because it does not requires management and controlling on lower level implementation at user level. Rapid growth of the cloud technology pushed numerous educational institutions to revise their IT infrastructure and follow the cloud development. Cloud computing entails many challenges related to the management of on-demand virtual infrastructures. One of these challenges is the automated provisioning of resources and services in cloud infrastructure. However, efficient resource provisioning is a key challenge for cloud computing and resolving such kind of problem can reduce under or over utilization of resources, increase user satisfaction by serving more users during peak hours, reduce implementation cost for providers and service cost for users. This architecture is intended to provision efficient and reliable resources and to enable users to select a predefined service, customize it according to user's requirements and deploy it automatically. To provision the services as a resource, it provisioned based on SLA.
327	Cloud computing provides facility to its customers to dynamically scale up the applications, platform and the hardware infrastructure. But the resources provided from one cloud provider are finite and at some point of time can violate the SLA (service level agreements). One approach can be used to better facilitate the customers is to scale the applications, software platforms and the infrastructure to multiple independent clouds i.e. federated clouds. The federated clouds can share the resources with other cloud providers as the scale and load increases and can pay for the service on usage based. Virtual machine allocation is also an important parameter of federated clouds, because multiple clouds are exchanging the VM (Virtual Machine) with one another and the trading policies of all the clouds is not same. VM Allocation can be optimized for cost-effective Virtual machine allocation. This paper is a survey of all VM allocation policies available in federated clouds.
332	As cloud computing has become a popular computing paradigm, many companies have begun to build increasing numbers of energy hungry data centers for hosting cloud computing applications. Thus, energy consumption is increasingly becoming a critical issue in cloud data centers. In this paper, we propose a dynamic resource management scheme which takes advantage of both dynamic voltage/frequency scaling and server consolidation to achieve energy efficiency and desired service level agreements in cloud data centers. The novelty of the proposed scheme is to integrate timing analysis, queuing theory, integer programming, and control theory techniques. Our experimental results indicate that, compared to a statically provisioned data center that runs at the maximum processor speed without utilizing the sleep state, the proposed resource management scheme can achieve up to 50.3\% energy savings while satisfying response-time-based service level agreements with rapidly changing dynamic workloads.
333	Cloud computing has attracted significant attention due to the increasing demand for low-cost, high performance, and energy-efficient computing. In this large-scale, heterogeneous, multi-user environment of a cloud system, profit maximization for the cloud service provider (CSP) is a key objective. In this paper, the problem of global optimization of the cloud system operation (in the sense of lowering operation costs by maximizing energy efficiency, while satisfying user deadlines defined in the Service Level Agreements) is addressed from the perspective of the CSP. The modeling of the workload dictates viable approaches toward cloud operation optimization. Of the two current models: independent batch requests and task graphs with dependencies, we adopt the later. This fine-grained treatment of workloads provides many opportunities for energy and performance optimizations, thus enabling the CSP to meet user deadlines at lower operation costs. However, these optimizations require additional efforts in terms of resource provisioning, virtual machine placement, and task scheduling. Such issues are addressed in a holistic fashion in the proposed framework. In this cloud environment, users can construct their own services and applications based on the available set of virtual machines, but are relieved from the burden of resource provisioning and task scheduling. The CSP will then capitalize on the data parallelisms in each user workload, effectively manage the collective user requests, and apply custom optimizations to create a global energy cost and deadline-aware cloud platform.
336	Efficient provisioning of resources is a challenging problem in cloud computing environments due to its dynamic nature and the need for supporting heterogeneous applications with different performance requirements. Currently, cloud datacenter providers either do not offer any performance guarantee or prefer static VM allocation over dynamic, which lead to inefficient utilization of resources. Earlier solutions, concentrating on a single type of SLAs (Service Level Agreements) or resource usage patterns of applications, are not suitable for cloud computing environments. In this paper, we tackle the resource allocation problem within a datacenter that runs different type of application workloads, particularly non-interactive and transactional applications. We propose admission control and scheduling mechanism which not only maximizes the resource utilization and profit, but also ensures the SLA requirements of users. In our experimental study, the proposed mechanism has shown to provide substantial improvement over static server consolidation and reduces SLA Violations.
337	Efficient provisioning of resources is a challenging problem in cloud computing environments due to its dynamic nature and the need for supporting heterogeneous applications. Even though VM (Virtual Machine) technology allows several workloads to run concurrently and to use a shared infrastructure, still it does not guarantee application performance. Thus, currently cloud datacenter providers either do not offer any performance guarantee or prefer static VM allocation over dynamic, which leads to inefficient utilization of resources. Moreover, the workload may have different QoS (Quality Of Service) requirements due to the execution of different types of applications such as HPC and web, which makes resource provisioning much harder. Earlier work either concentrate on single type of SLAs (Service Level Agreements) or resource usage patterns of applications, such as web applications, leading to inefficient utilization of datacenter resources. In this paper, we tackle the resource allocation problem within a datacenter that runs different types of application workloads, particularly non-interactive and transactional applications. We propose an admission control and scheduling mechanism which not only maximizes the resource utilization and profit, but also ensures that the QoS requirements of users are met as specified in SLAs. In our experimental study, we found that it is important to be aware of different types of SLAs along with applicable penalties and the mix of workloads for better resource provisioning and utilization of datacenters. The proposed mechanism provides substantial improvement over static server consolidation and reduces SLA violations.
347	The optimization problem addressed by this paper involves the allocation of resources in a private cloud such that cost to the provider is minimized (through the maximization of resource sharing) while attempting to meet all client application requirements (as specified in the SLAs). At the heart of any optimization based resource allocation algorithm, there are two models: one that relates the application level quality of service to the given set of resources and one that maps a given service level and resource consumption to profit metrics. In this paper we investigate the optimization loop in which each application�s performance model is dynamically updated at runtime to adapt to the changes in the system. These changes could be perturbations in the environment that had not been included in the model. Through experimentation we show that using these tracking models in the optimization loop will result in a more accurate optimization and thus result in the generation of greater profit.
349	The cloud computing as a ubiquitous paradigm could provide different services for internet users and Information Technology (IT) companies through datacenters that located around the world. However, cloud provider faces several problems such as high energy consumption and fault occurrence issues in cloud datacenters. Hence, cloud provider has to make a trade-off between energy and fault to gain more profit. In this paper, a migration method for the virtual machines to handle fault problem and prevent Service Level Agreement (SLA) violation is proposed by considering energy consumption constraints. The approach considers the lowest increasing in energy consumption and the minimal deadline miss ratio as the most significant factors for migration of each Virtual Machine (VM). The results show that low SLA violation is achieved with the least amount of energy consumption compared to other methods. It is also shown that the energy increasing by migration has an exponential relationship with the failure rate increasing.
360	SLAs are common means to define specifications and requirements of cloud computing services in business relationships. The terms that define the guaranteed availability for a given period are fundamental to these contracts. In this context, a natural question for cloud providers is: How to guarantee the availability promised? This paper studies the level of availability offered to a virtual machine during an SLA period in clouds with different: size, redundancy, and fault tolerance techniques. Finally, this paper proposes the use of the SLA -budget for the implementation of smart policies in: i) the assignment of spare servers when virtual machines are restored. ii) the dynamic use of different fault tolerance licenses. Using such policies results in a considerable reduction of the probability of breaching the SLA guarantee, by making an efficient use of the cloud resources available. This paper is a first step in the design of SLA-aware cloud architectures.
363	Cloud computing systems (e.g., hosting datacenters) have attracted a lot of attention in recent years. Utility computing, reliable data storage, and infrastructure-independent computing are example applications of such systems. Operational cost in these systems is highly dependent on the resource management algorithms used to assign virtual machines (VMs) to physical servers and possibly migrate them in case of power and thermal emergencies. Energy non-proportionality of IT devices in a datacenter, cooling system inefficiency, and power delivery network constraints should be considered by the resource management algorithms in order to minimize the energy cost as much as possible. Scalability of the resource assignment solution is one of the biggest concerns in designing these algorithms. This thesis examines the resource management problem in datacenters. First a centralized datacenter resource management is proposed, which considers service level agreements (SLAs) in VM placement in order to minimize the total operational cost of the datacenter. Second, a hierarchical SLA-based resource management structure is proposed, which considers the peak power constraints and cooling-related power consumption in addition to the scalability issue. The proposed hierarchical structure fits the hierarchical resource distribution in datacenters. The proposed structure is suitable to track and react to dynamic changes inside the datacenter to satisfy SLA constraints and avoid emergencies. Third, a load balancing algorithm to minimize the operational cost of a multi-datacenter cloud system is presented. Load balancing creates an opportunity to reduce the operational cost of the cloud system considering dynamic energy pricing and availability of green renewable power plants in a datacenter site.
364	Cloud computing systems (or hosting datacenters) have attracted a lot of attention in recent years. Utility computing, reliable data storage, and infrastructure-independent computing are example applications of such systems. Electrical energy cost of a cloud computing system is a strong function of the consolidation and migration techniques used to assign incoming clients to existing servers. Moreover, each client typically has a service level agreement (SLA), which specifies constraints on performance and/or quality of service that it receives from the system. These constraints result in a basic trade-off between the total energy cost and client satisfaction in the system. In this paper, a resource allocation problem is considered that aims to minimize the total energy cost of cloud computing system while meeting the specified client-level SLAs in a probabilistic sense. The cloud computing system pays penalty for the percentage of a client's requests that do not meet a specified upper bound on their service time. An efficient heuristic algorithm based on convex optimization and dynamic programming is presented to solve the aforesaid resource allocation problem. Simulation results demonstrate the effectiveness of the proposed algorithm compared to previous work.
	
366	Distributed computing systems have attracted a lot of attention due to increasing demand for high performance computing and storage. Resource allocation is one of the most important challenges in the distributed systems especially when the clients have some Service Level Agreements (SLAs) and the total profit depends on how the system can meet these SLAs. In this paper, an SLA-based resource allocation problem in a server cluster is considered. The objective is to maximize the total profit, which is the total price gained from serving the clients subtracted by the operation cost of the server cluster. The total price depends on the average request response time for each client as defined in their utility functions, while the operating cost is related to the total energy consumption. A joint optimization framework is proposed, comprised of request dispatching, dynamic voltage and frequency scaling (DVFS) for individual cores, as well as server-level and core-level consolidations. Each core in the cluster is modeled using a continuous-time Markov decision process (CTMDP). A near-optimal hierarchical solution is proposed, consisting of a central manager and distributed local agents. Each local agent employs linear programming-based CTMDP solving method to solve the DVFS problem for the corresponding core. The central manager solves the request dispatching problem and finds the optimal number of turned on cores and servers for request processing, thereby achieving a desirable tradeoff between service request response time and power consumption. Experimental results demonstrate that the proposed near-optimal resource allocation and consolidation algorithm consistently outperforms baseline algorithms.
367	With increasing demand for computing and memory, distributed computing systems have attracted a lot of attention. Resource allocation is one of the most important challenges in the distributed systems specially when the clients have Service Level Agreements (SLAs) and the total profit in the system depends on how the system can meet these SLAs. In this paper, an SLA-based resource allocation problem for multi-tier applications in the cloud computing is considered. An upper bound on the total profit is provided and an algorithm based on force-directed search is proposed to solve the problem. The processing, memory requirement, and communication resources are considered as three dimensions in which optimization is performed. Simulation results demonstrate the effectiveness of the proposed heuristic algorithm.
368	"With increasing demand for high performance computing and data storage, distributed computing systems have attracted a lot of attention. Resource allocation is one of the most important challenges in the distributed systems specially when the clients have some Service Level Agreements (SLAs) and the total profit in the system depends on how the system can meet these SLAs. In this paper, an SLA-based resource allocation problem for cloud computing is considered and a distributed solution to this problem is presented. The processing, data storage, and communication resources are considered as three dimensions in which optimizations are performed. Simulation results demonstrate that the proposed heuristic algorithm is robust (produces high quality solutions independent of the initial solution provided) and produces solutions very close to the ""optimum"" (best solution found by Monte Carlo simulation)."
377	In order to improve service execution in Clouds, the management of Cloud Infrastructure has to take measures to adhere to Service Level Agreements and Business Level Objectives, from the application layer through to how services are supported at the lowest hardware levels. In this paper a risk model methodology and holistic management approach is developed specific to the operation of the Cloud Infrastructure Provider and is applied through improvements to SLA fault tolerance in Cloud Infrastructure. Risk assessments are used to analyse execution specific data from the Cloud Infrastructure and linked to a business driven holistic management component that is part of a Cloud Manager. Initial results show improved eco-efficiency, virtual machine availability and reductions in SLA failure across the whole Cloud infrastructure by applying our combined risk-based fault tolerance approach.
379	
395	Cloud computing is a new computing mode after the technology of distributed computing, grid computing etc., which is to provide services according to a pay-as-you-go plan. But security and privacy problems is the big obstacle to the promoting of the cloud applications. In this paper, the challenges of privacy protection are discussed in detail. A cloud service privacy protection model based on the trusted third party is proposed to realize the agreement on privacy protection between the user and the cloud service provider and protect the user privacy effectively.
396	"Cloud computing brings in a novel paradigm to foster ITbased service economy in scalable computing infrastructures by allowing guaranteed on-demand resource allocation with flexible pricing models. Scalable computing infrastructures not only require autonomous management abilities but also the compliance to users' requirements through Service Level Agreements (SLAs). Such infrastructures should automatically respond to changing components, workload, and environmental conditions as well as prevent violations of agreed SLAs. The essential requirements for SLA-based orchestration of services include agile component-based infrastructure to support these orchestrations
407	Improving the utilization of resources and service qualities, and reducing the system energy consumption are two important goals of dynamic virtual machine management in cloud computing. Nevertheless, the reduction of energy consumption is inconsistent with the improvement of resource utilization. In order to get the balance, a new multi-objective decision-making method of virtual machine placement based on gray correlation degree is proposed, three factors like the energy consumption, Service Level Agreement (SLA) violation and server load are used as the evaluation indexes, and corresponding evaluation functions are biut for them, finally the multi-objective decision-making model of the virtual machine placement based on gray correlation degree is established. Evaluations via experiments show that the proposed method of virtual machine placement can achieve good results under most virtual machine selection policies.
415	In cloud datacenters, since resource requirements change frequently, how to assign and manage resources efficiently while meeting service level agreements (SLAs) of different types of applications is an important research issue. In this paper, we propose an Application-aware Resource Allocation (App-RA) scheme to predict resource requirements and allocate an appropriate number of virtual machines (VMs) for each application in SDN-based cloud datacenters. To the best of our knowledge, the proposed App-RA is the first application-aware resource allocation scheme that adapts to all types of applications. The App-RA can meet SLAs, allocate resources efficiently, and reduce power consumption for each application in cloud datacenters. The proposed App-RA adopts the neural network based predictor to forecast the requirements of resources (CPU, Memory, GPU, Disk I/O and bandwidth) for an application. In the proposed App-RA, we have designed two algorithms which allocate appropriate numbers of virtual machines and use the VM allocation threshold to avoid SLA violations for five different types of applications. In addition, we adopt an SDN-based OpenFlow network with CICQ switches to appropriately schedule packets for different types of application in the network layer. Finally, simulation results show that the power consumption of the proposed App-RA is only 9.21\% higher than that of the best case (oracle) and the power consumption of EAACVA, which is a representative resource allocation method for non-graphic applications, is 104.58\% worse than that of App-RA. Furthermore, the SLA violation rate of the proposed App-RA is less than 4\% for all applications.
422	In resource provisioning for cloud computing, an important issue is how resources may be allocated to an application mix such that the service level agreements (SLAs) of all applications are met. A performance model with two interactive job classes is used to determine the smallest number of servers required to meet the SLAs of both classes. For each class, the SLA is specified by the relationship: Prob [response time � x] � y. Two server allocation strategies are considered: shared allocation (SA) and dedicated allocation (DA). For the case of FCFS scheduling, analytic results for response time distribution are used to develop a heuristic algorithm that determines an allocation strategy (SA or DA) that requires the smallest number of servers. The effectiveness of this algorithm is evaluated over a range of operating conditions. The performance of SA with non-FCFS scheduling is also investigated. Among the scheduling disciplines considered, a new discipline called probability dependent priority is found to have the best performance in terms of requiring the smallest number of servers.
423	There are various significant issues in resource allocation, such as maximum computing performance and green computing, which have attracted researchers� attention recently. Therefore, how to accomplish tasks with the lowest cost has become an important issue, especially considering the rate at which the resources on the Earth are being used. The goal of this research is to design a sub-optimal resource allocation system in a cloud computing environment. A prediction mechanism is realized by using support vector regressions (SVRs) to estimate the number of resource utilization according to the SLA of each process, and the resources are redistributed based on the current status of all virtual machines installed in physical machines. Notably, a resource dispatch mechanism using genetic algorithms (GAs) is proposed in this study to determine the reallocation of resources. The experimental results show that the proposed scheme achieves an effective configuration via reaching an agreement between the utilization of resources within physical machines monitored by a physical machine monitor and service level agreements (SLA) between virtual machines operators and a cloud services provider. In addition, our proposed mechanism can fully utilize hardware resources and maintain desirable performance in the cloud environment.
425	We apply virtual machine abstractions to networked autonomous vehicles enabling what we call cloud computing in space. In analogy to traditional system virtualization and cloud computing, there are (customer-operated) virtual vehicles that essentially perform like real vehicles although they are hosted by possibly fewer, shared (provider-operated) real vehicles. Here the focus is, however, on motion rather than computation. In the service-level agreement, a virtual vehicle is a virtual machine plus a virtual speed. We define virtual deadline for each task based on virtual speed, and make the spatial cloud a soft real-time system [7], [1]. The performance isolation is measured by the average of tardiness and delivery probability. We use Voronoi tessellation to allocate the tasks to the real vehicles, and design scheduling policies such as Earliest Virtual Deadline First (EVDF), Earliest Dynamic Virtual Deadline First (EDVDF) and credit scheduling policy for the real vehicles. EVDF is shown to minimize the tardiness. Under EVDF, we identify a worst case arrival process maximizing tardiness. We show in simulation that abstracting real vehicles such as cars or planes to virtual vehicles enables virtual vehicles to move in space like real vehicles with guaranteed tardiness (e.g. � 1%) while being hosted by significantly fewer, e.g. 1-for-7.5, shared real vehicles.
428	Virtualization and cloud computing technologies now make it possible to consolidate multiple online services, which are packed in Virtual Machines (VMs), into a smaller number of physical servers. However, it is still a challenging scheduling problem for cloud provider to dynamically manage the VM allocation for handling variable workloads without Service Level Agreement (SLA) violation. In this paper, we propose a Migration-based Elastic Consolidation Scheduling (MECS) mechanism to automate elastic resource scaling for cloud systems. Different from the previous researches, we take both the dynamic workload fluctuation and the VM migration overhead into account. We first develop an online resource demand predictor, which is an ARIMA-based VM resource demand state predictor, to achieve adaptive resource allocation for cloud applications. We then propose a migration-based elastic consolidation scheduling heuristic to dynamically consolidate the VMs with adaptive resource allocation for reducing the number of physical machines. Extensive experiment results show that our scheduling is able to realize elastic resource allocation with acceptable effect on SLAs.
430	Consolidating virtual machine workload is a unique feature of cloud computing platforms that greatly reduces the operating cost of the cloud data center. Correctly consolidating VMs� workloads for a large scale cloud computing platform is nontrivial because a shortsighted scheme may save some cost in one aspect but becomes expensive in other aspects being neglected. In this paper, we present a framework that automates the VM consolidation process to improve the VMs and servers assignment whenever such improvement is possible. The proposed VM consolidation framework can achieve a balance among multiple administrative objectives (e.g., power cost, network cost) during the VM consolidation process. The solution method of solving the VM consolidation problem is designed based on the powerful and efficient semi quasi Mconvex optimization framework. The proposed algorithm can also produce VM consolidation solutions that require minimal system reconfigurations (e.g., VM migrations, turning on/off servers). More importantly, the proposed algorithm can be implemented distributedly so that the scalability of the proposed framework is greatly improved. As a result, the proposed framework is efficient, scalable and highly practical.
431	In nowadays computing clouds, it is of the cloud providers' economic interests to correctly consolidate the workload of the virtual machines (VMs) into the suitable physical servers in the cloud data center in order to minimize the total maintenance cost. However, during the consolidation process, sufficient protection should be provided to the service level agreement (SLA) of the VMs. In this paper, the VM consolidation problem for MapReduce enabled computing clouds has been investigated. In the MapReduce enabled computing clouds, MapReduce jobs are carried out by homogeneous MapReduce VM instances that have identical hardware resource. Two resource allocation schemes with corresponding SLA constraints for the MapReduce VMs and the non-MapReduce VMs are proposed. Based on these schemes, the VM consolidation problem is modeled as an integer nonlinear optimization problem and an efficient algorithm has been proposed to locate its solutions. The results show that better VM consolidation performance can be achieved by colocating MapReduce instances together with non-MapReduce instances in the same set of physical servers.
438	The advent of Science Clouds enables scientists to facilitate large-scale scientific computational experiments over cloud environment besides specialized supercomputers in diverse science domains. Cloud computing service elicits efficiency on on-demand resource usage and timely execution at any given time depending on experimental requirements. Hybrid clouds, composing of private and public clouds, even extend research opportunities on resource selection for further complicated experiments but increase the needs of dynamic resource management to maximize its utilization. At existing public cloud providers for commercial use, rule-based and schedule-based mechanisms have been tried for automatic resource allocation to provide resources for processing dynamic workload of modern applications. However, most of the auto-scaling methods just simply support performance metric such as CPU utilization but rarely are aware of Service Level Agreements (SLA) including execution deadline or cost. In this paper, we propose an auto-scaling method that automatically allocates resources depending on variable resource requirements in hybrid clouds satisfying a user's requirements on SLA. We present experimental results which show that the proposed auto-scaling can minimize SLA violations and acceptable cost if needed.
439	Last year's earthquake in Eastern Japan sparked a growing demand for redundant structures in Web 3-tier model/servers, NICs and physical lines - which are required by conventional on-premises environments with high reliability and availability - to be directly migrated to cloud environments. In such environments, when applying ICMP connectivity checks and conventional fault detection methods such as IP-SLA, it takes time to recover from faults to allow for coordination between the core network and the user logical networks in the cloud. This paper proposes a method where a correspondence list of user logical networks and core networks is registered beforehand in a cloud management system. When a fault has occurred, the NIC of the virtual machine corresponding to the fault location is closed off, thereby resulting in a rapid switchover to a redundant configuration.
441	Current service-level agreements (SLAs) offered by cloud providers make guarantees about quality attributes such as availability. However, although one of the most important quality attributes from the perspective of the users of a cloud-based Web application is its response time, current SLAs do not guarantee response time. Satisfying a maximum average response time guarantee for Web applications is difficult due to unpredictable traffic patterns, but in this paper we show how it can be accomplished through dynamic resource allocation in a virtual Web farm. We present the design and implementation of a working prototype built on a EUCALYPTUS-based heterogeneous compute cloud that actively monitors the response time of each virtual machine assigned to the farm and adaptively scales up the application to satisfy a SLA promising a specific average response time. We demonstrate the feasibility of the approach in an experimental evaluation with a testbed cloud and a synthetic workload. Adaptive resource management has the potential to increase the usability of Web applications while maximizing resource utilization.
444	"A Service-Level Agreement (SLA) provides surety for specific quality attributes to the consumers of services. However, current SLAs offered by cloud infrastructure providers do not address response time, which, from the user�s point of view, is the most important quality attribute for Web applications. Satisfying a maximum average response time guarantee for Web applications is difficult for two main reasons: first, traffic patterns are highly dynamic and difficult to predict accurately
446	In hosting environments such as IaaS clouds, desirable application performance is usually guaranteed through the use of Service Level Agreements (SLAs), which specify minimal fractions of resource capacities that must be allocated for unencumbered use for proper operation. Arbitrary colocation of applications with different SLAs on a single host may result in inefficient utilization of the host�s resources. In this paper, we propose that periodic resource allocation and consumption models � often used to characterize real-time workloads � be used for a more granular expression of SLAs. Our proposed SLA model has the salient feature that it exposes flexibilities that enable the infrastructure provider to safely transform SLAs from one form to another for the purpose of achieving more efficient colocation. Towards that goal, we present MORPHOSYS: a framework for a service that allows the manipulation of SLAs to enable efficient colocation of arbitrary workloads in a dynamic setting. We present results from extensive trace-driven simulations of colocated Video-on-Demand servers in a cloud setting. These results show that potentially-significant reduction in wasted resources (by as much as 60%) are possible using MORPHOSYS.
452	The load on today�s service-oriented systems is strongly varying in time. It is advantageous to conserve energy by adapting the number of replicas according to the recent load. Over-provisioning of service replicas is to be avoided, since it increases the operating costs. Under-provisioning of service replicas leads to serious performance degradation and violates service-level agreements. To reduce energy consumption and maintain appropriate performance, we study two service replication strategies: (1) arrival rate based and (2) response time based policy. By simulation, we show that the average number of service replicas and response time can be reduced especially when combining our proposed replication strategies and load balancing schemes.
457	With active deployment of virtualization in large scale data centers and cloud computing environments, allocation and scheduling of virtual and physical resources raise more challenges and may have negative impacts on system performance due to: (1) the isolation between the guest Virtual Machines (VMs) and the Virtual Machines Monitor (VMM), and (2) the independent and even conflicting operations between multiple VMs. In this paper a stochastic model of resources in virtualized environments is proposed and resource allocation and scheduling algorithm are proposed to provide performance guarantees and service differentiation in contending conditions. In the proposed algorithm user behavior and workloads are characterized through the historical and real time performance profiling and estimation from hosted agents within individual VMs. The resources are allocated according to the demand as well as the performance of the targeted VMs based on the Suffer age aggregation and performance feedback. Experiments on a real Xen based virtualization environment with 3 VMs are conducted and evaluated for accuracy, efficiency, sensitivity, and overhead. The results show that the performance feedback based allocation can achieve a higher SLA satisfaction rate as 97.5\%, a lower load imbalance index as 17.6\%. The results also show that this algorithm is valid, effective and scalable for implementation in real virtualized environments.
458	Cloud infrastructure providers deploy Dynamic Resource Management (DRM) to minimize the cost of datacenter operation, while maintaining the Service Level Agreement (SLA). Such DRM schemes depend on the capability to migrate virtual machine (VM) images. However, existing migration techniques are not suitable for highly utilized clouds due to their latency and bandwidth critical memory transfer mechanisms. In this paper, we propose guide-copy migration, a novel VM migration scheme to provide a fast and silent migration, which works nicely under highly utilized clouds. The guide-copy migration transfers only the memory pages accessed at the destination node in the near future by running a guide version of the VM at the source node and a migrated VM at the destination node simultaneously during the migration. The guide-copy migration's highly accurate and low-bandwidth memory transfer mechanism enables a fast and silent VM migration to maintain the SLA of all VMs in the cloud.
460	In the on-demand cloud environment, web application providers have the potential to scale virtual resources up or down to achieve cost-effective outcomes. True elasticity and cost-effectiveness in the pay-per-use cloud business model, however, have not yet been achieved. To address this challenge, we propose a novel cloud resource auto-scaling scheme at the virtual machine (VM) level for web application providers. The scheme automatically predicts the number of web requests and discovers an optimal cloud resource demand with cost-latency trade-off. Based on this demand, the scheme makes a resource scaling decision that is up or down or NOP (no operation) in each time-unit re-allocation. We have implemented the scheme on the Amazon cloud platform and evaluated it using three real-world web log datasets. Our experiment results demonstrate that the proposed scheme achieves resource auto-scaling with an optimal cost-latency trade-off, as well as low SLA violations.
473	In cloud computing, it remains a challenge to allocate virtualized resource with financial cost minimization and acceptable Quality of Service assurance. In general, the VM instance is allocated to cloud service users based on not actual job processing time but the fixed resource allocation time predetermined by cloud pricing policy in contrast to grid environment. In this case, the unnecessary cost dissipation is occurred by the wasted partial instance hours of allocated resource. To address this problem, we propose the heuristic based workflow scheduling scheme considering cloud-pricing model in this paper. Our scheme is composed of two phases: VM packing and MRSR (Multi Requests to Single Resource) phases. In VM-packing phase, preassigned multi tasks are aggregated into the common VM instance sequentially, and these tasks are merged in parallel by MRSR phase. By using our proposed schemes, we are able to reduce the number of required VM instances and achieve the significant cost saving while we guarantee the user's SLA (Service Level Agreement) in terms of workflow deadline. Our proposed schemes cannot only reduce the cost by 30% compared to traditional workflow scheduling schemes but also assure user's SLA.
480	With the fast growth of (online) content and the need for high quality content services, cloud data centers are increasingly becoming the preferred places to store data and retrieve it from. With a highly variable network traffic and limited resources, efficient server selection and data transfer rate allocation mechanisms become necessary. However, current approaches rely on random server selection schemes and inefficient data transmission rate control mechanisms. In this paper we present SCDA, an efficient server selection, resource allocation and enforcement mechanism with many salient features. SCDA has prioritized rate allocation mechanism to satisfy different service level agreements (SLA)s on throughput and delays. The allocation scheme can achieve max/min fairness. SCDA has a mechanism to detect and hence mitigate SLA violation in realtime. We have implemented SCDA in the NS2 simulator. Extensive experimental results confirm some of the design goals of SCDA to obtain a lower content transfer time and a higher throughput. The design of SCDA can achieve a content transfer time which is about 50% lower than the existing schemes and a throughput which is higher than existing approaches by upto than 60%.
483	Increasingly, applications are moving into the cloud, which is actually supported by large-scale data centres on the ground. These data centres are complex systems to manage and centralized solutions might not be able to meet the required scale nor make an efficient use of their networks. In this paper, we propose a hierarchical approach to dynamic resource management in data centres, where we leverage the topology of the data centre network to design the management hierarchy. We define a set of aggregate metrics at various levels in the hierarchy to convey system state information to higher management levels, and define managers' responsibilities and interactions. We evaluate our proposed approach through simulation. Experiments show that this management approach greatly reduces the flow of management data across the data centre network, thus reducing network overhead.
492	The Cloud Computing utility model has raised a number of challenges particularly in relation to service provisioning and user requirements. In such a service driven environment it is very important that resource provisioning can be optimized and users and applications have some level of assurance that their requirements can be satisfied. However, in order to guarantee a certain level of Quality of Service (QoS) Service Level Agreements (SLA) which specify contracts between providers and users are commonly used. This paper aims to investigate the QoS issues and SLA management within Cloud Computing and to present a framework for QoS assurance.
494	A key issue in cloud computing environments is to maximize profit by accepting all incoming requests and to minimize SLA violation that causes penalty for cloud providers. Achieving these goals highly depends on optimum usage of available resources in data-centers. So far other works have focused on provisioning QoS in the cloud. In this work as an alternative to others we describe shortcomings caused by the lack of resource management mechanism and propose a mathematical model for this issue to better describe it. Then we propose a heuristic algorithm to improve resource management and finally we use fuzzy systems to apply our expertise knowledge to solve the problem. Experimental results on eucalyptus as a private cloud shows that proposed algorithms with suitable management of resources result in an increment of profit by reducing rejected requests of the cloud.
497	
500	Server consolidation is very attractive for cloud computing platforms to improve energy efficiency and resource utilization. Advances in multi-core processors and virtualization technologies have enabled many workloads to be consolidated in a physical server. However, current virtualization technologies do not ensure performance isolation among guest virtual machines, which results in degraded performance due to contention in shared resources along with violation of service level agreement (SLA) of the cloud service. In that sense, minimizing performance interference among co-located virtual machines is the key factor of successful server consolidation policy in the cloud computing platforms. In this work, we propose a performance model that considers interferences in the shared last-level cache and memory bus. Our performance interference model can estimate how much an application will hurt others and how much an application will suffer from others. We also present a virtual machine consolidation method called swim which is based on our interference model. Experimental results show that the average performance degradation ratio by swim is comparable to the optimal allocation.
504	"Cloud computing is a new technology which is proffering IT services based on pay-as-you-go model to consumers from everywhere in the world. The growing demand of Cloud infrastructure and modern computational requests like business, scientific and web applications result in large-scale data centers and lead to extra electrical energy consumption. High energy consumption causes high operational cost and also led to high carbon emission which is harmful for atmosphere. Hence, energy-efficient techniques are required to minimize the negative effects of Cloud computing on the environment. Virtual machines (VMs) migration, dynamic consolidation in the virtualized data centers in cloud environments and switching idle physical machines off could yield reduce energy consumption
510	Delivering Internet-scale services and IT-enabled capabilities as computing utilities has been made feasible through the emergence of Cloud environments. While current approaches address a number of challenges such as quality of service, live migration and fault tolerance, which is of increasing importance, refers to the embedding of users� and applications� behaviour in the management processes of Clouds. The latter will allow for accurate estimation of the resource provision (for certain levels of service quality) with respect to the anticipated users� and applications� requirements. In this paper we present a two-level generic black-box approach for behavioral-based management across the Cloud layers (i.e., Software, Platform, Infrastructure): it provides estimates for resource attributes at a low level by analyzing information at a high level related to application terms (Translation level) while it predicts the anticipated user behaviour (Behavioral level). Patterns in high-level information are identified through a time series analysis, and are afterwards translated to low-level resource attributes with the use of Artificial Neural Networks. We demonstrate the added value and effectiveness of the Translation level through different application scenarios: namely FFMPEG encoding, real-time interactive e-Learning and a Wikipedia-type server. For the latter, we also validate the combined level model through a trace-driven simulation for identifying the overall error of the two-level approach.
517	Presently Cloud Computing is on high demand as it provides a way to reduce the cost of building infrastructure through virtualization of resources. Virtualization of resources requires a highly dynamic resource management mechanism. As cloud computing provides the facility to the cloud users to send multiple request simultaneously, there must be a self managing/provisioning scheme that all resources are made available to the requesting users in the efficient manner to satisfy their requirement and for improvement of resource utilization. In this paper we proposed an efficient framework named called EARA (Efficient Agent based Resource Allocation) for resource allocation based on agent computing on SaaS level in Cloud Computing. EARA Contain five different agents, each agent equipped with functionality to collect information regarding all resources available in actual cloud deployment based on signed SLA agreement, and then replies to the user with appropriate allocation or response code.
521	Cloud computing is a very popular area in current world which is rising very fast and the futures of the field seems really broad and strong. In order to provide quality of service in the cloud environment is a highly challenging task. The cloud clients should obtain reliable services from the provider based on their desire. In the particular cloud computing service, the resource allocation process is based on quality of service and cost of resource. The provider should allocate the resources in a proper way to render good services to the clients. This paper elucidates an elegant survey made on the different resource allocation methods used in the cloud computing environment.
525	Enterprise Cloud Computing has the complicated issues of the multi-tenancy, cross layer service composition, i.e., Software as a Service, Platform as a Service, and Infrastructure as a Service, multiple constraints from user requirements and Service Level Agreements. To assure the quality of service and effectiveness of Enterprise Cloud Computing, a middleware to support the service composition and monitoring in Enterprise Cloud Computing is highly important. We have designed a middleware for Enterprise Cloud Computing which can automatically manage the resource allocation of services from services, platforms, and infrastructures, and provide a cost-effective and secure way to access services from cloud environment. This architecture will compose of several functions, such as Service Monitoring, Service Composition, and Service Status Analysis. In order to provide complete transparency of the underlying technology and the surrounding environment, thus easy for management and testing, the architecture we proposed in this paper employs the agent technology to handle the monitoring of requested Quality of Service requirements and Service Level Agreement, which are capable to support the Verification and Validation, and, furthermore, to dynamically analyze resources allocation and deployment.
529	Infrastructure-as-a-Service (IaaS) platforms, such as Amazon EC2, allow clients access to massive computational power in the form of virtual machines (VMs) known as instances. Amazon hosts three different instance purchasing options, each with its own service level agreement covering availability and pricing. In addition, Amazon offers access to a number of geographical regions, zones, and instance types from which to select. In this paper, we present a resource allocation and job scheduling framework (RAMC-DC), which utilizes Amazon's rich selection of service offerings---particularly within Spot and On-Demand instance purchasing options---aiming to cost efficiently execute deadline-constrained jobs. The framework is capable of ensuring quality of service in terms of cost, deadline compliance and service reliability. Such capacities are realized incorporating a set of novel strategies including execution time and cost approximation, bidding and resource allocation strategies. To the best of our knowledge, RAMC-DC most extensively exploits the service diversity of Amazon EC2, and offers a comprehensive cost efficiency solution that is able to deliver both the performance and reliability of On-Demand instances and the low costs of Spot instances. Experimental results obtained from extensive simulations using Amazon's Spot price traces show that our approach keeps deadline breaches and early-termination rates as low as 0.47\% and 0.18\%, respectively. This reliable performance is achieved with total costs between 13\% and 20\% of an equivalent approach using only On-Demand instances.
533	Cloud computing can provide services by aggregating, selecting, and sharing of geographically distributed heterogeneous resources, in a fully virtualized manner. Themost important problem in Cloud computing is that the geographic distributed resources owned by different institutions with their different price models, usage policies and changing load. Meanwhile, the availability of resources and the load on them dynamically varies with time. Hence, resource management in Clouds is a complicated task. An economicbased method with service-level agreement (SLA) restriction is presented to allocate Cloud resources, which is based on Pareto optimality theory and realizes the optimal allocation of Cloud resources. This paper describes a Cloud bank model that depends on market mechanism to understand deeply Pareto optimality.
536	Resource scheduling based on SLA (Service Level Agreement) in cloud computing is NP-hard problem. There is no efficient method to solve it. This paper proposes a new method to solve the problem by applying stochastic integer programming for optimal resource scheduling in cloud computing. Applying Gr�bner bases theory for solving the stochastic integer programming problem and the experimental results of the implementation are also presented.
539	Cloud computing aims at reducing energy consumption and maximizing resource efficiency without violating service level agreement (SLA). To address these important issues, this study proposes an energy-efficient resource provisioning technology with SLA consideration for virtual machine scheduling. According to SLAs, the resource manager could consolidate virtual machines onto the physical machine for meeting customers SLA requests. Experimental results show that the proposed approach outperforms other proposed ones in power consumption.
542	The prominence of cloud computing that provides resources on demand to various types of users including enterprises as well as engineering and scientific institutions is growing rapidly. An effective resource management middleware is necessary to harness the power of the underlying distributed hardware in a cloud. The resource manager needs to be able to effectively perform mapping (matchmaking and scheduling) of user requests (jobs) on to resources to satisfy desired system objectives as well as user's requirements for a quality of service that is often captured in a service level agreement (SLA). This paper concerns the problem of meeting an end-to-end SLA (characterized by an earliest start time, an execution time, and a deadline) for applications that require service from multiple resources (referred to as multi-stage applications) on a system subjected to an open stream of request arrivals. A new budget-based algorithm and a resource manager called MapReduce Budget-based Resource Manager (MRBB-RM) are devised for effectively performing matchmaking and scheduling of an open stream of MapReduce jobs (a popular multi-stage application) with SLAs on a distributed environment such as a cloud or a cluster. A detailed description of the algorithm and its performance analysis are presented.
543	In this paper we study the important issue of verifying Service Level Agreement (SLA) in a semi-trusted (or untrusted) cloud. Cloud computing services promise elastic computing and storage resources in a pay-as-you-go way. A SLA between a cloud service provider (CSP) and a user is a contract which specifies the resources and performances that the cloud should offer. However, the CSP has the incentive to cheat on SLA, e.g., providing users with less CPU and memory resources than that specified in the SLA, which allows the CSP to support more users and make more profits. A malicious CSP can disrupt the existing SLA monitoring/verification techniques by interfering the monitoring/measurement process. Therefore, we present a SLA verification framework that leverages a third party auditor (TPA). Under the TPA framework, we propose an effective testing algorithm that can detect SLA violations of physical memory size in virtual machine (VM). Using real experiments, we show that the algorithm can detect cloud cheating on VM memory size (i.e., SLA violations). Furthermore, our algorithm can defend various attacks from a malicious CSP, which tries to hide a SLA violation.
551	Effective virtualized resource provisioning is one of the most challenging tasks of the cloud computing system. Especially when visits to an online service are rapidly increasing, state-of-the-art approaches cannot response to the workload changes in time. In this paper we introduce SPRNT, a framework that accelerates the resource provisioning. By using SPRNT, the cloud system can quickly adapt to the explosively increasing workload and ensure that the SLA is not violated. The experimental results show that SPRNT can deal with the tremendously increasing of the workload and can also reduce the over-provisioned resources efficiently and effectively.
560	"Large scale distributed computing infrastructures pose challenging resource management problems, which could be addressed by adopting one of two perspectives. On the one hand, the problem could be framed as a global optimization that aims to minimize some notion of system-wide (social) cost. On the other hand, the problem could be framed in a game-theoretic setting whereby rational, selfish users compete for a share of the resources so as to maximize their private utilities with little or no regard for system-wide objectives. This game-theoretic setting is particularly applicable to emerging cloud and grid environments, testbed platforms, and many networking applications. This thesis considers both perspectives.
563	One of the major benefits of cloud computing is virtualization scaling. Compared to existing studies on virtual machine scaling, this paper introduces Hurst exponent which gives additional characteristics for data trends to supplement the often used Markov transition approach. This approach captures both the long and short-term behaviors of the virtual machines (VMs). The dataset for testing of this approach was gathered from the computer usage of key servers supporting a large university. Performance evaluation shows our approach can assist prediction of VM CPU usage toward effective resource allocation. In turn, this allows the cloud resource provider to monitor and allocate the resource usage of all VMs in order to meet the service level agreements for each VM client.
570	Cloud computing aims to provide dynamic leasing of server capabilities as scalable virtualized services to end users. However, data centers hosting cloud applications consume vast amounts of electrical energy, thereby contributing to high operational costs and carbon footprints. Green cloud computing solutions that can not only minimize the operational costs but also reduce the environmental impact are necessary. This study focuses on the Infrastructure as a Service model, where custom virtual machines (VMs) are launched in appropriate servers available in a data center. A complete data center resource management scheme is presented in this paper. The scheme can not only ensure user quality of service (through service level agreements) but can also achieve maximum energy saving and green computing goals. Considering that the data center host is usually tens of thousands in size and that using an exact algorithm to solve the resource allocation problem is difficult, the modified shuffled frog leaping algorithm and improved extremal optimization are employed in this study to solve the dynamic allocation problem of VMs. Experimental results demonstrate that the proposed resource management scheme exhibits excellent performance in green cloud computing.
573	Resource management is a challenging issue in cloud computing. This paper aims to allocate requested jobs to cloud resources suitable for cloud user requirements. To achieve the aim, this paper proposes an ontology-based job allocation algorithm for cloud computing to perform inferences based on semantic meanings. We extract resource candidates depending on user requirements and allocate a job to the most suitable candidate for an agreed Service Level Agreement (SLA). The cloud ontology allows the proposed system to define concepts and describe their relations. Hence, we can process complicated queries for searching cloud resources. To evaluate performance of our system, we conducted some experiments compared with the existing resource management algorithms. Experimental results verify that the ontology-based resource management system improves the efficiency of resource management for cloud computing.
585	"With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, Cloud computing is defined and the architecture for creating Clouds with market-oriented resource allocation are provided by leveraging technologies such as Virtual Machines (VMs). Also insights on market-based resource management strategies that encompass both customer-driven service management and computational have been provided.
596	With the emergence of Cloud Computing resources of physical machines have to be allocated to virtual machines (VMs) in an ondemand way. However, the efficient allocation of resources like memory, storage or bandwidth to a VM is not a trivial task. On the one hand, the Service Level Agreement (SLA) that defines QoS goals for arbitrary parameters between the Cloud provider and the customer should not be violated. On the other hand, the Cloud providers aim to maximize their profit, where optimizing resource usage is an important part. In this paper we develop a simulation engine that mimics the control cycle of an autonomic manager to evaluate different knowledge management techniques (KM) feasible for efficient resource management and SLA attainment. We especially focus on the use of Case Based Reasoning (CBR) for KM and decision-making. We discuss its suitability for efficiently governing on-demand resource allocation in Cloud infrastructures by evaluating it with the simulation engine.
607	IaaS is commonly offered by Cloud vendors where VMs can be created and run on cloud resources. The resource allocation for each VM is defined by the user and the VM is created on a physical machine where ample resources exist to support the VMs operation at the maximum expected load. There are a number of opportunities for improvement when allocating host resources to VMs. VM-resident applications are often n-tier, with different VMs responsible for parts of the distributed application. The network proximity to the user may be an issue for some applications. Resource allocation to VMs should also be such that, rather than a user over-provisioning the VM, the VMs minimal operational requirements are specified so that the VM can be resource-throttled at times of heavy load. This paper presents a system called Innkeeper, a three-tiered distributed approach which aims to dynamically allocate resources to VMs in a way that ensures SLA compliance, with consideration given to the network conditions between related VMs. Initial experiments show that the InnKeeper framework provides a means of real-time, SLA-compliant allocation of distributed cloud resources.
615	Cloud computing is aimed at offering elastic resource allocation on demand in a pay-as-you-go fashion to cloud consumers. To achieve this goal in automatic manner, a resource scaling mechanism is needed that maintains application performance according to Service Level Agreements (SLA) and reduces resource costs at the same time. In this paper, we present a cross-correlation prediction approach based on machine learning that predicts resource demands of multiple resources of virtual machines running in a cloud infrastructure. Based on these predictions, a proactive resource allocation scheme is applied that assigns only the required resources to virtual machines to keep their cost to a minimum. Experimental results with the web serving multi-tier application benchmark of CloudSuite show the effectiveness of our approach compared to a non-cross-correlation prediction technique in achieving better prediction accuracy and better application performance.
625	In this paper, we present our analysis of resource scheduling to achieve SLA-aware profit optimization in cloud services. This work is part of a real system called Intelligent Cloud Database Coordinator (ICDC), which is being built at the NEC Labs to manage very large cloud service delivery infrastructures. As we present in our performance evaluation, the analysis shows that the choice of resource scheduling policy makes a significant difference in SLA delivery and total profit of the cloud service provider.
630	Energy efficiency is becoming a very important concern for Cloud Computing environments. These are normally composed of large and power consuming data centers to provide the required elasticity and scalability to their customers. In this context, many efforts have been developed to balance the loads at host level. However, determining how to maximize the resources utilization at Virtual Machine (VM) level still remains as a big challenge. This is mainly driven by very dynamic workload behaviors and a wide variety of customers' resource utilization patterns. This paper introduces a dynamic resource provisioning mechanism to overallocate the capacity of real-time Cloud data centers based on customer utilization patterns. Furthermore, its impact on the trade-off between energy efficiency and SLA fulfillment is analyzed. The main idea is to exploit the resource utilization patterns of each customer to decrease the waste produced by resource request overestimations. This creates the opportunity to allocate additional VMs in the same host incrementing its energy efficiency. Nevertheless, this also increases the risk of QoS affectations. The proposed model considers SLA deadlines, predictions based on historical data, and dynamic occupation to determine the amount of resources to overallocate for each host. In addition, a compensation mechanism to adjust resource allocation in cases of underestimation is also described. In order to evaluate the model, simulation experimentation was conducted. Results demonstrate meaningful improvements in energy-efficiency while SLA-deadlines are slightly impacted. However, they also point the importance of strongest compensation policies to reduce availability violations especially during peak utilization periods.
633	User satisfaction as a significant antecedent to user loyalty has been highlighted by many researchers in market based literatures. SLA violation as an important factor can decrease users' satisfaction level. The amount of this decrease depends on user's characteristics. Some of these characteristics are related to QoS requirements and announced to service provider through SLAs. But some of them are unknown for service provider and selfish users are not interested to reveal them truly. Most the works in literature ignore considering such characteristics and treat users just based on SLA parameters. So, two users with different characteristics but similar SLAs have equal importance for the service provider. In this paper, we use two user's hidden characteristics, named willingness to pay for service and willingness to pay for certainty, to present a new proactive resource allocation approach with aim of decreasing impact of SLA violations. New methods based on learning automaton for estimation of these characteristics are provided as well. To validate our approach we conducted some numerical simulations in critical situations. The results confirm that our approach has ability to improve users' satisfaction level that cause to gain in profitability.
634	Many datacenters employ server consolidation to maximize the efficiency of platform resource usage. As a result, multiple virtual machines (VMs) simultaneously run on each datacenter platform. Contention for shared resources between these virtual machines has an undesirable and non-deterministic impact on their performance behavior in such platforms. This paper proposes the use of shared resource monitoring to (a) understand the resource usage of each virtual machine on each platform, (b) collect resource usage and performance across different platforms to correlate implications of usage to performance, and (c) migrate VMs that are resource-constrained to improve overall datacenter throughput and improve Quality of Service (QoS). We focus our efforts on monitoring and addressing shared cache contention and propose a new optimization metric that captures the priority of the VM and the overall weighted throughput of the datacenter. We conduct detailed experiments emulating datacenter scenarios including on-line transaction processing workloads (based on TPC-C) middle-tier workloads (based on SPECjbb and SPECjAppServer) and financial workloads (based on PARSEC). We show that monitoring shared resource contention (such as shared cache) is highly beneficial to better manage throughput and QoS in a cloud-computing datacenter environment.
640	As the number of existing cloud vendors rises, resource count and types are ever increasing leading to a need of cloud management solutions which facilitate easy cloud adoption. While providing several services, cloud management's primary role is resource provisioning. In order to meet application needs in terms of resources, cloud developers must carefully choose among the existing offers in order to deploy their applications. The research presented in this paper enables developers to automate the process of resource provisioning by specifying their preference towards resources and resource attributes based on which the system can propose solutions for their requirements.
645	As more enterprises and companies utilize cloud environments and services to reduce IT costs, cloud service providers have to minimize their operation cost of data centers in order to gain advantages when competing with each other. In this study, an efficient virtual machine provisioning mechanism for cloud data center is introduced. Based on CloudSim 3.0 simulator, our experimental results have shown that the proposed provisioning mechanism is very efficient in terms of CPU utilization and produces less number of virtual machine migrations in comparison with existing provisioning mechanisms such as LrMmt [1].
653	Cloud platforms host several independent applications on a shared resource pool with the ability to allocate computing power to applications on a per-demand basis. The use of server virtualization techniques for such platforms provide great flexibility with the ability to consolidate several virtual machines on the same physical server, to resize a virtual machine capacity and to migrate virtual machine across physical servers. A key challenge for cloud providers is to automate the management of virtual servers while taking into account both high-level QoS requirements of hosted applications and resource management costs. This paper proposes an autonomic resource manager to control the virtualized environment which decouples the provisioning of resources from the dynamic placement of virtual machines. This manager aims to optimize a global utility function which integrates both the degree of SLA fulfillment and the operating costs. We resort to a constraint programming approach to formulate and solve the optimization problem. Results obtained through simulations validate our approach.
658	The performance of a computing cloud depends on its agile response to the user needs and in the quality of the service provided. SLA assurance will increase user's confidence in the system by guaranteeing the cloud resources needed to users at certain point in the future. Negotiation between the cloud provider and consumer is necessary to achieve SLA. Haizea is an open source lease manager which can act as a scheduler for OpenNebula. It uses resource leases as resource allocation abstraction and executes these leases by allocating virtual machines. Haizea does not support negotiation currently. This work implements negotiation process using Haizea to provide SLA based lease admission. The experiment results show that the proposed method increases system utilization and maximizes the number of leases accepted as compared to Haizea's default policies.
663	In resource provisioning for datacenters, an important issue is how resources may be allocated to an application such that the service level agreements (SLAs) are met. Resource provisioning is usually guided by intuitive or heuristic expectation of performance and existing user model. Provisioning based on such methodology, however, usually leads to more resources than are actually necessary. While such overprovisioning may guarantee performance, this guarantee may come at a very high cost. A quantitative performance estimate may guide the provider in making informed decisions about the right level of resources, so that acceptable service performance may be provided in a cost-effective manner. A quantitative estimate of application performance must consider its workload characteristics. Due to the complex workload characteristics of commercial software, estimation of its performance and provisioning to optimize for cost is not straightforward. In this work we looked at breaking the application into isolated modalities (modality is a scenario in which an application is used, for example, instant messaging, and voice calls are two different modalities of a media application) and measuring resource cost per modality as an effective methodology to provision datacenters to optimize for performance and minimize cost. When breaking the application into modalities, resource cost is assessed in isolation. Results are then aggregated to estimate the overall resource provisioning requirements. A validation tool is used to simulate the load and validate the assumptions. This was applied to a commercially available solution and validated in a datacenter setting.
664	In resource provisioning for datacenters, an important issue is how resources may be allocated to an application such that the service level agreements (SLAs) are met. Resource provisioning is usually guided by intuitive or heuristic expectation of performance and existing user model. Provisioning based on such methodology, however, usually leads to more resources than are actually necessary. While such overprovisioning may guarantee performance, this guarantee may come at a very high cost. A quantitative performance estimate may guide the provider in making informed decisions about the right level of resources, so that acceptable service performance may be provided in a cost-effective manner. A quantitative estimate of application performance must consider its workload characteristics. Due to the complex workload characteristics of commercial software, estimation of its performance and provisioning to optimize for cost is not straightforward. In this work we look at breaking an application into isolated modalities. We measured resource cost per modality and validate as an effective methodology to provision datacenters to optimize for performance and minimize cost.
	
667	Current service platform offers that provide Infrastructure as a Service (IaaS) do not adequately meet the requirements expressed by interactive real-time services. Online application response times can not yet be enforced in virtual infrastructures without service level objectives (SLOs) that meet virtual machine interconnection constraints. This paper presents a framework spanning from the service description model over the IaaS platform interface for service level agreement (SLA) negotiation to the management of virtual network resources in an IaaS environment.
668	Recently, message brokering services that are based on publish and subscribe paradigm are widely adopted in many areas such as remote facilities monitoring. Typically, they support a large scale distributed network of exchanging messages between services and clients. In this paper, we introduce a cloud-based message brokering service which allows better scalability and dynamic load balancing between brokers. Our system framework monitors each virtual machine (VM)'s resources and in order to meet the service level agreement (SLA), a VM is rented or recollected automatically by the system. The framework also provides the best available broker that a client can use for connections. Our experimental results show that our system framework can achieve up to 58.60\% of scalability improvement at the maximum number of subscriber connections.
670	The increase acceptance and adoption of cloud computing has seen many research projects focusing on tradition distributed computing issues such as resource allocation and performance. The scalability and dynamic heterogeneity of cloud computing presents a different challenge in deciding how resources are allocated to services. In this paper we identify the nature of cloud computing dynamics with virtualisation as a key part to resource allocation and meeting QoS needs. Service Level Agreement (SLA) mainly equates to actions taken when best effort falls and does not address the significant needs of meeting QoS demands as well as efficient utilisation of resources.
679	With the increasing use of computing in our day to day life it seems that Computing is 5th utility in our life after water, electricity, gas, and telephony. Cloud computing is a technology that host and deliver services over the Internet. Since the cloud computing has lots of benefits including on demand service, pay per use facility, multi- tenancy, device and location independency etc. a question definitely comes into mind, actually from where it has emerged so that it has this much of benefits. So clearly the answer is, that it does not owes its origin from a single technology there are lots of technologies that are working behind this. The technologies working behind the cloud computing are cluster computing, grid computing, peer-to-peer (P2P) computing, SOA, autonomic computing, map reduce etc. Also cloud computing has some features of utility computing and virtualization. From all above said technologies virtualization forms the core of cloud computing. So in this paper we are going to talk about virtualization i.e. what actually virtualization is, what are its various features because of which it acts as the main technology for cloud computing, what are its various types, how virtualization can be achieved etc. As the main aim of the virtualization is to utilize more and more hardware resources i.e. cpu, memory, storage etc, so under given load conditions whenever we change the configuration of virtual machines there will definitely some effect on the performance of cloud virtual machine, that is why in this paper we are also going to give the brief introduction to various parameters that will be affected by the amount of load on virtual machine using which we can measure or monitor the performance of virtual machines.
680	Cloud computing is a rising distributed computing paradigm that promises to offer cost effective, scalable, on demand services to users without the need for large investments in purchasing and managing the infrastructure. Cloud computing brings significant benefits for both service providers and users because of its characteristics. With virtualization - A key technology for cloud computing, service providers can ensure isolation of multiple user workloads, provision resources in cost effective manner i.e. low resource allocation at lower load and quickly scale up according to load conditions thus optimally utilizing resources.
685	Cloud Computing is a type of computing which can be considered as a new era of computing. Cloud can be considered as a rapidly emerging new paradigm for delivering computing as a utility. In cloud computing various cloud consumers demand variety of services as per their dynamically changing needs. So it is the job of cloud computing to avail all the demanded services to the cloud consumers. But due to the availability of finite resources it is very difficult for cloud providers to provide all the demanded services. From the cloud providers' perspective cloud resources must be allocated in a fair manner. So, it's a vital issue to meet cloud consumers' QoS requirements and satisfaction. This paper mainly addresses key performance issues, challenges and techniques for resource allocation in cloud computing. It also focuses on the key issues related to these existing resource allocation techniques and summarizes them.
689	Cloud computing is an emerging technology in the IT world. Some features of cloud, such as low cost, scalability, robustness and availability are attracting large-scale industries as well as small businesses towards cloud. A virtual machine (VM) is a software that can run its own operating system and applications just like an operating system on a physical computer. As the number of users increases, allocation of resources and scheduling become a complex task in a cloud. In a federated cloud environment when resource requirements of user requests exceed resource limits of cloud provider, to fulfil the requests the cloud provider can out-source to other cloud providers' resources. Under these circumstances it is desirable to minimize the Service Level Agreement (SLA) violations. This can be achieved through load balancing. This paper proposes a load balancing algorithm that is threshold based. We consider two types of pricing models for VMs, on-demand and reserved. Simulation results show that the proposed algorithm reduces the SLA violations.
690	Cloud computing delivers platform, software application and hardware infrastructure as a service over Internet. It allows the users to utilize the service on-demand and pay-per-use model as given by Amazon EC2. There are two types of cost involved, fixed and variable. Cost will rise as the demand increases. In this paper the web service request coming from users are categorized into groups and virtual machines (VM's) are created group wise from the physical servers. Mapping of these two group id is done by application provisioner. Fixed cost is charged on requesting reserved resource as per Service Level Agreement (SLA) and variable cost for requesting instantaneous resources. Ultimate cost charged to the user is minimized by 16\%. Efficient workload monitoring, grouping of VM's and user request helps to finish user request with in time and better utilization of available resources (physical serves). Idle time of resource is reduced from 50\% to 23\%.
695	Today Cloud computing is on demand as it offers dynamic flexible resource allocation, for reliable and guaranteed services in pay-as-you-use manner, to Cloud service users. So there must be a provision that all resources are made available to requesting users in efficient manner to satisfy their needs. This resource provision is done by considering the Service Level Agreements (SLA) and with the help of parallel processing. Recent work considers various strategies with single SLA parameter. Hence by considering multiple SLA parameter and resource allocation by preemption mechanism for high priority task execution can improve the resource utilization in Cloud. In this paper we propose an algorithm which considered Preemptable task execution and multiple SLA parameters such as memory, network bandwidth, and required CPU time. An obtained experimental results show that in a situation where resource contention is fierce our algorithm provides better utilization of resources.
696	Today Cloud computing is on demand as it offers dynamic flexible resource allocation, for reliable and guaranteed services in pay-as-you-use manner, to Cloud service users. So there must be a provision that all resources are made available to requesting users in efficient manner to satisfy their needs. This resource provision is done by considering the Service Level Agreements (SLA) and with the help of parallel processing. Recent work considers various strategies with single SLA parameter. Hence by considering multiple SLA parameter and resource allocation by preemption mechanism for high priority task execution can improve the resource utilization in Cloud. In this paper we propose an algorithm which considered Preemptable task execution and multiple SLA parameters such as memory, network bandwidth, and required CPU time. An obtained experimental results show that in a situation where resource contention is fierce our algorithm provides better utilization of resources.
697	Cloud computing have attracted a lot of attention recently due to increasing demand for high performance computing and storage. Resource allocation is one of the most important challenges in the cloud computing system especially when the clients have some Service Level Agreements (SLAs) and the total profit depends on how the system can meet these SLAs. Moreover, a data center typically hosts and manages a suite of application environments and a fixed number of servers that are allocated to these application environments in a way that maximizes a certain utility function. In this paper, we consider the problem of SLA-based joint optimization of application environment assignment, request dispatching from the clients to the servers, as well as resource allocation in a data center comprised of heterogeneous servers. The objective is to maximize the total profit, which is the total price gained from serving the clients subtracted by the operation cost of the data center. The total price depends on the average service request response time for each client as defined in their utility functions, while the operating cost is related to the total energy consumption. We propose a near-optimal solution of the joint optimization problem based on the Hungarian algorithm for the assignment problem, as well as convex optimization techniques, in a way that is similar to the constructive partitioning algorithm in VLSI computer-aided design (CAD). Experimental results demonstrate that the proposed nearoptimal joint application environment assignment and resource allocation algorithm outperforms baseline algorithms by up to 65.7\%.
698	Pervasive use of cloud computing and the resulting rise in the number of datacenters and hosting centers (which provide platform or software services to clients who do not have the means to set up and operate their own compute facilities) have brought forth many concerns including the electrical energy cost, peak power dissipation, cooling, carbon emission, etc. With power consumption becoming an increasingly important issue for the operation and maintenance of the hosting centers, corporate and business owners are becoming increasingly concerned. Furthermore, provisioning resources in a cost-optimal manner so as to meet different performance criteria such as throughput or response time has become a critical challenge. The goal of this paper is to provide an introduction to resource provisioning and power/thermal management problems in datacenters and to review strategies that maximize the datacenter energy efficiency subject to peak/total power consumption and thermal constraints while at the same time meeting stipulated service level agreements in terms of task throughput and/or response time.
705	Recently the massive growth of mobile devices has led to a significant change in the users' computer and Internet usage, along with the dramatic development of mobile services, or mobile computing. Mobile devices, also known as thin clients, limited by either their capacities (CPU, memory or battery) or their network resources, do not always meet users' satisfaction in using mobile services. As a solution to this problem, the thin clients should be connected to other devices with more powerful computing or network resources such as computers and laptops, etc.(thick client), so that the former can capitalize on the latter to strengthen their ability to perform computing tasks. There were a number of related studies in minimizing the limitation of thin client based on the same idea, yet none have been found efficient. In this paper, we present a new method that bases its architecture on the thin-thick client collaboration. We further introduce a strategy to optimize the data distribution, especially big data in cloud computing. Moreover, we propose algorithms to allocate resources to meet service level agreement (SLA) and quality of service (QoS). After a lot of simulations have been conducted and intensively evaluated, the results show that our approach can improve resource allocation efficiency and has better performance than the existing ones.
708	The Internet of Things is an emerging paradigm shaping our current understanding about the future of Internet. Most of today's inter-enterprise applications follow the distributed computing paradigm in which parts of the application are executed on different network-interconnected computers. The paper presents a state-of-the art review with a particular focus on resource management architectures, models and algorithms in Clouds and inter-Clouds for energy-efficient message delivery and covers the existing methods that support fault tolerance. The paper also presents a critical overview for existing methods focusing on delimiting the area of resource management, resource allocation, scalability, and fault tolerance in Clouds and inter-Clouds. The simulation will be an instrument for theoretical validation and a critical analysis of Cloud Simulators and identify the main benefits of each one will represent another important results.
711	Cloud Computing is emerging as the next generation platform which would facilitate the user on pay as you use mode as per requirement. The primary aim of Cloud Computing is to provide efficient access to remote and geographically distributed resources with the help of Virtualization in Infrastructure as a Service (IaaS). We need different kind of virtual machines (VM) as per the requirement and cloud provider provides these services as per the Service Level Agreement (SLA) to ensure QoS. For managing large amount of VM requests, the cloud providers require an efficient resource scheduling algorithm. In this paper, a comparative study has been made for different types of VM scheduling and provisioning algorithms and are briefly discussed and analyzed. Then we can conclude that one of these algorithms is batter for scheduling and provisioning with the perspective of cost and security of VMs.
712	Data processing on the cloud is increasingly used for offering cost effective services. In this paper, we present a method for resource allocation for data processing services over the cloud taking into account not just the processing power and memory requirements, but the network speed, reliability and data throughput. We also present algorithms for partitioning data, for doing parallel block data transfer to achieve better throughput and allocated cloud resources. We also present methods for optimal pricing and determination of Service Level Agreements for a given data processing job. The usefulness of our approach is shown through experiments performed under different resource allocation conditions.
714	Cloud computing is evolving into the default operational framework running modern data centers. Efficient data center operation is concerned with the total amount of energy consumed as well as assuring adequate resources are available to process all of the incoming work requests. Existing research has demonstrated several algorithms that can be used to determine the optimal number of resources required to service these requests. However, a key issue not addressed in these algorithms is determining the frequency of recalculating the number of required resources. Changing the required resources at a rate slower than the optimal update frequency results in lower energy efficiency due to the over allocation of resources. Changing the resources at a rate higher than the optimal frequency results in insufficient time for systems to change state, which results in SLA violations. In this paper, a stochastic optimization model is presented that determines the optimal update frequency for changing the states of the nodes of the cloud as well as determining the proper frequency for recalculating the maximum expected load, which improves the determination of the optimum number of resources required, therefore maximizes energy efficiency and minimizes SLA violations.
715	Cloud computing is an emergent paradigm that allows customers to rent infrastructure, platforms and software as a service. With resource sharing and reuse through virtualization technology, cloud environments become even more cost effective and flexible. Nevertheless, networking within virtualized cloud still presents some challenges in performance and resource allocation. In this paper, we propose DBA-VM, a Dynamic Bandwidth Allocator for Virtual Machines with regard to the established SLAs. The proposed scheme enforces the isolation between the virtual machines through the transmission bandwidth adjustment at the network I/O channel. The experimental performance evaluation shows that DBA-VM allows to the virtualized system to respect each virtual machine SLA while reducing the global physical resources (CPU and memory) consumption.
725	Cloud computing is a talented technology in today's market. Single cloud service providers could not offer quality services to user's requirements when workload becomes very high. Federated cloud mechanism helps to resolve the difficulties present in the single cloud service model. Better Resource Provisioning to user's requirements is one of the difficult tasks in federated cloud mechanism. Cloud Brokers takes the responsibility of resource provisioning in federated cloud model. Cloud brokers make use of the concept of cloud ranking to choose the best cloud service provider among the number of cloud service providers who satisfies the user requirements. Based upon the cloud ranking cloud broker assigns the user task to the selected cloud service provider. Up to now there is no standard method for cloud ranking. The proposed framework measures the quality and ranks the cloud service providers.
726	Resource scheduling is a momentous task of determining and formulating when an activity should start or end depending on the number of tasks, the availability of the resources, time and processor speed. A cloud is an aggregation of resources or services that are provided as service through network. In earlier version of cloud computing (grid computing) it was enough to find the subset of resources for the applications whereas cloud computing goes one step higher allocating resources to Virtual Machines (VM's) as well as scheduling tasks on the VM's. So resource scheduling is indispensable in cloud computing for load balancing and maximizing the utilization while minimizing the execution time and energy. Few approaches relating to energy salvation have certain disadvantages such as, time complexity and slow Convergence. Hence Energy-Aware Multi objective Chiropteran Algorithm (EAMOCA) is developed by bringing together the echo-localization and hibernation properties for scheduling resources as well as conserving energy. Promotion of energy salvation in cloud environment is achieved in a well delineated manner. By using the performance metrics such as total energy consumed by physical resources, SLA violation (CPU performance) and VM migration, we have manipulated our approach through real time implementation by setting up a private cloud employing VMware.
732	The commoditization of high performance interconnects, like 40+ Gbps InfiniBand, and the emergence of low-overhead I/O virtualization solutions based on SR-IOV, is enabling the proliferation of such fabrics in virtualized datacenters and cloud computing platforms. As a result, such platforms are better equipped to execute workloads with diverse I/O requirements, ranging from throughput-intensive applications, such as `big data' analytics, to latency-sensitive applications, such as online applications with strict response-time guarantees. Improvements are also seen for the virtualization infrastructures used in data center settings, where high virtualized I/O performance supported by high-end fabrics enables more applications to be configured and deployed in multiple VMs - VM ensembles (VMEs) - distributed and communicating across multiple datacenter nodes. A challenge for I/O-intensive VM ensembles is the efficient management of the virtualized I/O and compute resources they share with other consolidated applications, particularly in lieu of VME-level SLA requirements like those pertaining to low or predictable end-to-end latencies for applications comprised of sets of interacting services. This paper addresses this challenge by presenting a management solution able to consider such SLA requirements, by supporting diverse SLA-aware policies, such as those maintaining bounded SLA guarantees for all VMEs, or those that minimize the impact of misbehaving VMEs. The management solution, termed Distributed Resource Exchange (DRX), borrows techniques from principles of microeconomics, and uses online resource pricing methods to provide mechanisms for such distributed and coordinated resource management. DRX and its mechanisms allow policies to be deployed on such a cluster in order to provide SLA guarantees to some applications by charging all the interfering VMEs `equally' or based on the `hurt', i.e. amount of I/O performed by the VMEs. While these mechanisms are gen- ral, our implementation is specifically for SR-IOV-based fabrics like InfiniBand and the KVM hypervisor. Our experimental evaluation consists of workloads representative of data-analytics, transactional and parallel benchmarks. The results demonstrate the feasibility of DRX and its utility to maintain SLA for transactional applications. We also show that the impact to the interfering workloads is also within acceptable bounds for certain policies.
735	Cloud computing referred to as the on demand technology because it offers dynamic and versatile resource allocation for reliable and warranted services in pay as-you-use manner to public. It is a technology that uses the web and central remote servers to take care of data and applications and permits users to use applications without installation and access their personal files at any computer with the assistance of internet access. This technology allows rather more efficient computing by consolidative data storage, processing and bandwidth. The specialty of this technology is that any variety of cloud services can be simultaneously accessed by any variety of users. So it is necessary that every user should get sufficient resources in a well-organized manner. The resource allocation in cloud computing is nothing but integrating the cloud provider activities in order to utilize and allocate scarce resources. The service level agreement satisfaction is incredibly necessary concerning the user as well as the service provider. Minimum SLA violation brings most client satisfaction. Here in this paper a survey is meted out on the realm of resource management strategies that tries to preserve the customer satisfaction to its maximum. There are some metrics which are able to evaluate the potency of these resource allocation strategies. The deserves and demerits of each technique are also mentioned.
741	Cloud computing is a paradigm for large-scale distributed computing that makes use of existing technologies such as virtualization, service-orientation, and grid computing. In cloud environment, pool of virtual resources is always changing. Thus allocating the resources for particular job in this dynamic environment is challenging. Objective of this paper is to present a load balancing, specifically resource allocation scheme in cloud computing environment. This paper proposes a novel load balancing algorithm for allocation of the jobs. Allocation is made on the basis of the requirement submitted by the consumers or clients and finally a service level agreement is made between cloud service provider and cloud consumer.
745	With the users and the types of application on the cloud computing platform increasing, it becomes a critical problem about how to use the resources in the system effectively to ensure service level agreements (SLA). The load balancing algorithm is an important means to achieve efficient utilization of resources. This paper presents a dynamic load balancing algorithm based on virtual machine migration under cloud computing environment. The algorithm proposed the trigger strategy based on the fractal methods. The strategy determines the timing of the virtual machine migration through forecasting the timing to determine the timing of the virtual machine migration, which can avoid the problem of the peak instantaneous load trigger. Comparing with the other algorithms, such as ant colony and honey bees, this algorithm can achieve using of resources more balance. Finally, we test the system and the experimental results show that the algorithm can achieve load balancing and improve system performance.
750	Cloud-based ERP solutions, providing business processes automation and improving visibility across the entire organization, are becoming and an extremely popular alternative to traditional ERPs. One of the important challenges faced by cloud service providers is the effective management of cloud services performance. The ultimate goal of a cloud service provider is the maximization of its profit through reducing the number of quality of service (QoS) violations and decreasing service costs. The effective resource provisioning is still a challenging task for cloud computing providers because of the high variability of workload over time. On the one hand, cloud providers can respond to most of the queries owning only a restricted amount of resources, but this results in customers rejection during peak hours. On the other hand, valley hours incur in under-utilization of the resources, which forces the providers to increase their prices to be profitable. This paper represents cloud ERP query flow control model, built in Powersim, supporting cloud provider's decision-making process of resource allocation in order to provide SLA-aware profit optimization, based on query flow control and cloud services demand forecast.
754	SLA violations are typically viewed as service failures. If service fails once, it will fail again unless remedial action is taken. In a virtualized environment, a common remedial action is to restart or reboot a virtual machine (VM). In this paper we present, a VM live-migration policy that is aware of SLA threshold violations of workload response time, physical machine (PM) and VM utilization as well as availability violations at the PM and VM. In the migration policy we take into account PM failures and VM (software) failures as well as workload features such as burstiness (coefficient of variation or CoV >1) which calls for caution during the selection of target PM when migrating these workloads. The proposed policy also considers migration of a VM when the utilization of the physical machine hosting the VM approaches its utilization threshold. We propose an algorithm that detects proactive triggers for remedial action, selects a VM (for migration) and also suggests a possible target PM. We show the efficacy of our proposed approach by plotting the decrease in the number of SLA violations in a system using our approach over existing approaches that do not trigger migration in response to non-availability related SLA violations, via discrete event simulation of a relevant case study.
756	Cloud is one of the rapidly developing technologies as it allows convergence of many new areas like scalability, rapid elasticity and broad network access in cloud services. This paper focuses on resource allocation for the parallel data processing, which also occupies a major issue in cloud computing. The current paper aims to study the two major cloud problems i.e. QOS constrained resource allocation and parallel data management problems. The customer demands the cloud provider to host their application with desired SLA such as throughput, response time among others. We have proposed a methodology which is expected to achieve the specified SLA requirements in parallel data management and forecast/identifies the load to be processed by the cloud. This may use the divide and conquer strategy to predict the load on VM�s (in dividing phase) and which will be allocated to that requested customer. We take Nephele, a parallel data processing framework in which job graph issued by the customer to be converted into an execution graph on an Iaas cloud system as an example to frame our architecture resulting with minimum response time and efficient load balancing service.
758	"When data from multiple sources (sensors) are processed over a shared distributed computing infrastructure, it is necessary to often provide some Quality of Service (QoS) guarantees to each data stream. Service Level Agreements (SLAs) identify the cost that a user must pay to achieve the required QoS, and a penalty that must be paid to the user in case the QoS cannot be met. Assuming the maximisation of the revenue as the provider's objective, then it must decide which streams to accept for storage and analysis
762	The Cloud computing paradigm offers the illusion of infinite resources accessible to end-users anywhere at anytime. In such dynamic environment, managing distributed heterogeneous resources is challenging. A Cloud workload is typically decomposed into advance reservation and on-demand requests. Under advance reservation, end-users have the opportunity to reserve in advance the estimated required resources for the completion of their jobs without any further commitment. Thus, Cloud service providers can make a better use of their infrastructure while provisioning the proposed services under determined policies and/or time constraints. However, estimating end-users resource requirements is often error prone. Such uncertainties associated with job execution time and/or SLA satisfaction significantly increase the complexity of the resource management. Therefore, an appropriate resource management by Cloud service providers is crucial for harnessing the power of the underlying distributed infrastructure and achieving high system performance. In this paper, we investigate the resource provisioning problem for advance reservation under a Pay-as-you-Book pricing model. Our model offers to handle the extra-time required by some jobs at a higher price on a best-effort basis. However, satisfying these extra-times may lead to several advance reservations competing for the same resources. We propose a novel economic agent responsible for managing such conflicts. This agent aims at maximizing Cloud service provider revenues while complying with SLA terms. We show that our agent achieves higher return on investment compared to intuitive approaches that systematically prioritize reserved jobs or currently running jobs.
767	"This paper presents a Markovian analytical model to estimate service response time for elastic cloud applications. Given the expected application workload, the number of virtual machine (VM) instances, and the capacity of each VM instance, the model can approximate the mean service time. The mean service time is a critical metric to estimate, and contributes to the SLA end-to-end response time experienced by application users. The end-to-end response time is an aggregated delay of the service time in addition to delays incurred at the network nodes and links. Our analytical model focuses on estimating the mean service time
768	Cloud federation allows individual cloud providers dynamically collaborate to offer services to their end-users with the Quality of Service (QoS) targets agreed in the Service Level Agreements (SLA). However, the current federated cloud models are not QoS-oriented or SLA-aware. This paper proposes a QoS-oriented federated cloud computing framework where multiple independent cloud providers can cooperate seamlessly to provide scalable QoS-assured services and discusses a high level architecture of the federation components. The distinct features of the proposed federation framework is its QoS-orientation that can trigger the on-demand resource provisioning across multiple providers, hence helping to maximize QoS targets and resources usage, eliminate SLA violations and enhance SLA formalization.
770	Patterns of above- and below-ground biomass allocation in seedlings of nine common cloud forest (CF) tree species of western Mexico were examined under varying controlled light conditions using artificial shade houses. We analysed the relationships between vital rates (growth and survival) and four morphological traits (SLA, biomass allocation to stems, leaves and roots). We hypothesised that these traits represent differentiation axes in the way seedlings face the heterogeneous light regime typical of the CF understorey. For all species, traits between the different light levels, i.e. allocation to leaves, roots and stems differed among light levels. Five species had the largest SLA in the lowest light levels at the end of the experiment (Citharexylum, Dendropanax, Fraxinus, Quercus and Magnolia). Juglans was the only species with a large SLA at the highest light level (377.47cm2g_1). In contrast, light levels did not cause any significant variation in SLA of Persea and Simplococarpon at the end of the experiment. The relative height growth rates (RHGR) of the seedlings of five species were significantly different between light levels (P<0.05). Overall, all species grew better in the highest light levels. The RHGR of three species were correlated positively with SLA. In turn, allocation to stem, leaves and root biomass were strongly correlated with the RHGR of five species (e.g. Citharexylum, Dendropanax and Fraxinus). Survival did not vary significantly between treatments in any species, only in the case of Simplococarpon (P<0.05) and was correlated with all morphological variables. For this species, Peto and Peto's test showed a significantly larger survival of seedlings in the highest light level. The mean responses of these species based on all traits to the controlled light variation did not differed significantly. Our results show that these species display a wide range of resource allocation patterns when exposed to the varying light conditions that may be found in the forest understorey and highlight the role of morphological traits in this variation.
775	Cloud computing is increasingly being adopted in different scenarios, like social networking, business applications, scientific experiments, etc. Relying in virtualization technology, the construction of these computing environments targets improvements in the infrastructure, such as power-efficiency and fulfillment of users' SLA specifications. The methodology usually applied is packing all the virtual machines on the proper physical servers. However, failure occurrences in these networked computing systems can induce substantial negative impact on system performance, deviating the system from ours initial objectives. In this work, we propose adapted algorithms to dynamically map virtual machines to physical hosts, in order to improve cloud infrastructure power-efficiency, with low impact on users' required performance. Our decision making algorithms leverage proactive fault-tolerance techniques to deal with systems failures, allied with virtual machine technology to share nodes resources in an accurately and controlled manner. The results indicate that our algorithms perform better targeting power-efficiency and SLA fulfillment, in face of cloud infrastructure failures.
784	Running emerging main-memory database systems within virtual machines causes huge overhead, because these systems are highly optimized to get the most out of bare metal servers. But running these systems on bare metal servers results in low resource utilization, because database servers often have to be sized for peak loads, much higher than the average load. Instead, we propose to deploy them within light-weight containers that allow to control resource usage and to make use of spare resources by temporarily running other applications on the database server using virtual machines (VMs). The servers on which these VMs would normally run can be suspended, to save energy costs. But current database systems do not handle dynamic changes to resource allocation well and accurate estimates on resource demand are required to maintain SLAs. We focus on emerging main-memory database systems that support the mixed workloads of today's business intelligence applications and propose an cooperative approach in which the DBMS communicates its resource demand, gets informed about currently assigned resources and adapts its resource usage accordingly. We analyze the performance impact on the database system when spare resources are used by VMs and monitor SLA compliance.
787	As various consumers tend to use personalized Cloud services, Service Level Agreements (SLAs) emerge as a key aspect in Cloud and Utility computing. The objectives of this doctoral research are 1) to support a flexible establishment of SLAs that enhances the utility of SLAs for both providers and consumers, and 2) to manage Cloud resources to prevent SLA violations. Because consumers and providers may be independent bodies, some mechanisms are necessary to resolve different preferences when they establish a SLA. Thus, we designed a Cloud SLA negotiation mechanism for interactive and flexible SLA establishment. The novelty of this SLA negotiation mechanism is that it can support advanced multi-issue negotiation that includes time slot and price negotiations. In addition, to prevent SLA violations, we provided a SLA-driven resource allocation scheme that selects a proper data center among globally distributed centers operated by a provider. Empirical results showed that the proposed SLA negotiation mechanism supports faster agreements and achieves higher utilities. Also, the proposed SLA-driven resource allocation scheme performs better in terms of SLA violations and the provider's profits.
792	Cloud computing is a novel computing model, involving resource outsourcing with infinite and flexible resource scalability and �plug and play� provisioning. The recent materialization of this new computing model has radically changed everyone's perception of infrastructure paradigm, development models and delivery of software. Earlier people were reluctant to go for Clouds due to doubt on performance so since its inception main emphasis has been given on performance so that SLA is not violated. So resources were over provisioned to meet peak demands which resulted in underutilization of servers in off peak hours. This resulted in high power consumption, high carbon footprints and high cooling cost. We are proposing an energy efficient framework for Virtual Machine management which will result in minimum no. of servers used which will further result in low power consumption, low carbon footprints and which will make Cloud computing economical for both consumers and providers. Our model differs from others since we have restricted CPU utilization to a threshold limit as it has been experimentally proved that power consumption increases drastically when CPU utilization increases beyond 70\%. Moreover it also gives room for scaling up resources during peak hours.
797	"With the advent of gaining popularity in cloud computing, there is an increase in demand of resources among the heterogeneous workload types on cloud. Resource management is a key challenging problem that is faced by the cloud service providers, by achieving business goals and agreed level of service with the subscribers. This paper focuses on dynamic resource allocation with risk analysis by meeting service level agreements. Further proposed framework handles heterogeneous workload types by dynamic capacity planning with risk assessment to maximise the profit and resource utilisation on clouds. In addition to advanced resource reservation, SLA-based scheduling&#47
815	The number of cloud service users has increased worldwide, and cloud service providers have been deploying and operating data centers to serve the globally distributed cloud users. The resource capacity of a data center is limited, so distributing the load to global data centers will be effective in providing stable services. Another issue in cloud computing is the need for providers to guarantee the service level agreements (SLAs) established with consumers. Whereas various load balancing algorithms have been developed, it is necessary to avoid SLA violations (e.g., service response time) when a cloud provider allocates the load to data centers geographically distributed across the world. Considering load balancing and guaranteed SLA, therefore, this paper proposes an SLA-based cloud computing framework to facilitate resource allocation that takes into account the workload and geographical location of distributed data centers. The contributions of this paper include: (1) the design of a cloud computing framework that includes an automated SLA negotiation mechanism and a workload- and location-aware resource allocation scheme (WLARA), and (2) the implementation of an agent-based cloud testbed of the proposed framework. Using the testbed, experiments were conducted to compare the proposed schemes with related approaches. Empirical results show that the proposed WLARA performs better than other related approaches (e.g., round robin, greedy, and manual allocation) in terms of SLA violations and the provider's profits. We also show that using the automated SLA negotiation mechanism supports providers in earning higher profits.
817	Cloud computing service provides elastic and infinite computing resources to meet users' QoS requirements such as deadline and service cost. However, some tasks may be delayed because of the long waiting time before their resources are available. So, how to supply dynamic resource scaling and task migration is an important question which determines users' Service-level agreement(SLA) whether can be achieved. In this work, we construct the Torque Cloud management system to support dynamic task scheduling in the Cloud through applying Torque distributed resource management software to Eucalyptus Cloud platform. And also an idle resource cached dynamic scheduling algorithm(Idle Cached) is proposed to dynamically adjust tasks, which will take full advantage of idle resources in the resource pool. The results prove that the algorithm can quickly meet tasks' resource demands, minimize users' service cost and system energy-consumption.
832	Cloud computing is a new business computing model. The environment of the resources is very complex. The resources are physically distributed and connected by the network. There are many risks existing in the resource transactions. So how to make sure that the cloud computing platform can avoid these risks during the transactions and assure the quality of services (QoS) provided to the consumers is a very important issue in cloud computing. Service Level Agreement (SLA) is proposed to solve the problems between the customers and service suppliers. Cloud Bank model [1] is a resource management model based on economic principles and aims at solving all the commercial level problems in cloud computing. This paper presents a framework of Service Level Agreement based on the Cloud Bank's liquidity risk [2] predicting model. This SLA can help the Cloud Bank avoid the risks and assure the QoS to the consumers.
833	In our previous work, we have already introduce Finite Time Response-HTTP (FTR-HTTP), simple mechanism which have ability to guarantee quality of service (QoS) of web application for non-real time application. Our consideration at this approach based on many QoS methods at TCP/IP were implemented on network layer that have consequently at changing the network configuration or devices. At this paper, we would enhance our previous method to make FTR-HTTP can work and communicate with server that implemented virtual machine as usually used at cloud computing. We create architecture on server to implement FTR-HTTP and elaborate with server performance. With this architecture, application on cloud environment could be guarantee more strictly. Our hypothesis is this method could maintain response time under desired value based on each Service Level Agreement (SLA).
836	SlapOS is the first open source operating system for Distributed Cloud Computing. SlapOS is based on a grid computing daemon - called slapgrid - which is capable of installing any software on a PC and instantiate any number of processes of potentially infinite duration of any installed software. Slapgrid daemon receives requests from a central scheduler - the SlapOS Master - which collects back accounting information from each process. SlapOS Master follows an Enterprise Resource Planning (ERP) model to handle at the same time process allocation optimization and billing. Recent research on Cloud Computing has focused on the implementation of Service Level Agreements (SLA) and operation of large Data Centers. However, in case of Force Majeure such as natural disaster, strike, terrorism, unpreventable accident, etc., Service Level Agreements (SLA) no longer apply. Rather than centralizing Cloud Computing resources in large data centers, Distributed Cloud Computing resources are aggregated from a grid of standard PCs hosted in homes, offices and small data centers.
837	
839	Maximizing consolidation ratio, the number of virtual machines (VMs) in a physical machine, without violating customers' SLAs is an important goal in the cloud. We show that it is difficult to achieve this goal with existing hypervisor schedulers. The schedulers control only the amount of resource allocation, but not the sequence of VM execution. This sequence can significantly impact the response time when requests arrive concurrently for the VMs sharing the same CPU. We find that the response time can increase as much as 100\% for every additional VM in the system, even if the utilization does not exceed the maximum capacity. Therefore, existing schedulers have to reduce the consolidation ratio to meet SLAs. Previous resource-provisioning works rely on existing schedulers that cannot guarantee SLAs without reducing the consolidation ratio. We propose SageShift, a system that can achieve SLAs without penalizing the consolidation ratio. SageShift consists of a VM admission control - Sage, and a hypervisor scheduler - Shift. To admit a VM, Sage assesses feasibility of its SLA based on the patterns of incoming requests. Shift maintains the admitted SLAs by adjusting both the amount of resource allocation and the sequence of VM execution. The dynamic adjustment is based on the observed response time and the SLAs. We modify the KVM scheduler in Linux kernel to implement Shift. We show that Shift can improve the consolidation ratio by 66\% without compromising the SLAs. Under bursty incoming requests, Shift maintains all SLAs within 3\% of the percentile target. But existing schedulers in VMware ESXi, Xen and KVM fail to meet one or more SLAs with up to 33\% below the percentile target. Shift is also work-conserving. It allows best-effort VMs to run in the background in order to maximize hardware utilization without impacting SLAs.
840	Cloud computing is a new paradigm for the delivery of IT services. It has enabled many promising opportunities for features that cannot be easily implemented in traditional IT environments, such as elastic scalability, self-service deployment, resiliency and recovery, and so forth. Benchmarking the cloud requires a well-defined set of cloud performance metrics that should be able to sensitively distinguish the capabilities of cloud systems that enable those features. One way of defining benchmark metrics is based on observations of the internal mechanisms in a cloud. For example, an elasticity evaluation may be based on measuring a resource provisioning interval in the cloud. However, a more meaningful evaluation should be based on user-centric metrics. In this article, we will introduce a set of performance metrics that can be directly measured, calculated and compared by the cloud users, including workload consumers and the users who deploy and manage the workload life cycles. We will also discuss ways to organize the user-centric metrics, with different emphasis, into a benchmark that represents different use cases.
846	Cloud computing nowadays becomes quite popular among a community of cloud users by offering a variety of resources. However, burstiness in user demands often dramatically degrades the application performance. In order to satisfy peak user demands and meet Service Level Agreement (SLA), efficient resource allocation schemes are highly demanded in the cloud. However, we find that conventional load balancers unfortunately neglect cases of bursty arrivals and thus experience significant performance degradation. Motivated by this problem, we propose new burstiness-aware algorithms to balance bursty workloads across all computing sites, and thus to improve overall system performance. We present a smart load balancer, which leverages the knowledge of burstiness to predict the changes in user demands and on-the-fly shifts between the schemes that are �greedy� (i.e., always select the best site) and �random� (i.e., randomly select one) based on the predicted information. Both simulation and real experimental results show that this new load balancer can adapt quickly to the changes in user demands and thus improve performance by making a smart site selection for cloud users under both bursty and non-bursty workloads.
852	Performance assurance has become an important aspect in grid and cloud computing which provide services over the Internet, and Service Level Agreements (SLA) are frequently contracted between users and the service providers. However, the I/O performance of the storage or data access service is still provided on a best effort basis. Some distributed storage systems implement performance reservation, but the reservation is implemented inside of the storage and works in an adaptive manner. In order to promise performance guarantees to users, we propose a distributed storage system allowing application users to explicitly make an advanced and time-based reservation for I/O access and storage space. Thus the requested performance is guaranteed during the reserved time. This paper describes our proposed concept and the design architecture of the storage system, including the reservation interface, resource management and I/O control frameworks. Then it explains our prototype which implements a simple resource allocation strategy and I/O control of the storage network along the design. The experiment results using the prototype are also shown. They indicate that the reservation cost entailed only a low performance impact on users, and that the requested performance was achieved by the reservation feature.
857	Energy management has become increasingly necessary in large-scale cloud data centers to address high operational costs and carbon footprints to the environment. In this work, we combine three management techniques that can be used to control cloud data centers in an energy-efficient manner: changing the number of virtual machines, the number of cores, and scaling the CPU frequencies. We present a feedback controller that determines an optimal configuration to minimize energy consumption while meeting performance objectives. The controller can be configured to accomplish these goals in a stable manner, without causing large oscillations in the resource allocations. To meet the needs of individual applications under different workload conditions, the controller parameters are automatically adjusted at runtime based on a system model that is learned online. The potential of the proposed approach is evaluated in a video encoding scenario. The results show that our combined approach achieves up to 34\% energy savings compared to the constituent approaches�core change, virtual machine change, and CPU frequency change policies, while meeting the performance target.
874	A key issue for Cloud Computing data-centers is to maximize their profits by minimizing power consumption and SLA violations of hosted applications. In this paper, we propose a resource management framework combining a utility-based dynamic Virtual Machine provisioning manager and a dynamic VM placement manager. Both problems are modeled as constraint satisfaction problems. The VM provisioning process aims at maximizing a global utility capturing both the performance of the hosted applications with regard to their SLAs and the energy-related operational cost of the cloud computing infrastructure. We show several experiments how our system can be controlled through high level handles to make different trade-off between application performance and energy consumption or to arbitrate resource allocations in case of contention.
875	Cloud platforms host several independent applications on a shared resource pool with the ability to allocate computing power to applications on a per-demand basis. The use of server virtualization techniques for such platforms provide great flexibility with the ability to consolidate several virtual machines on the same physical server, to resize a virtual machine capacity and to migrate virtual machine across physical servers. A key challenge for cloud providers is to automate the management of virtual servers while taking into account both high-level QoS requirements of hosted applications and resource management costs. This paper proposes an autonomic resource manager to control the virtualized environment which decouples the provisioning of resources from the dynamic placement of virtual machines. This manager aims to optimize a global utility function which integrates both the degree of SLA fulfillment and the operating costs. We resort to a constraint programming approach to formulate and solve the optimization problem. Results obtained through simulations validate our approach.
877	
890	Cloud storage has recently appeared as a promising solution to handle the immense volume of data produced in nowadays' rich and ubiquitous digital environment. Existing approaches, however, do not fully address certain significant issues, such as Service Level Agreements, which tend to be rudimentary, and management of cloud systems, which often lacks sophistication. In this paper, we propose the definition and development of management models that systematically describe the resources, services, usage, and requirements in the context of a cloud storage environment. Furthermore, we introduce content oriented SLA Schemas, which are adapted according to the content term involved in each SLA. Combining all this often heterogeneous information in a smart, structured, and uniform manner can provide significant added value, allowing correlations, links and extraction of useful inferences. We thus propose to incorporate these models in a rich Unified Management Model, which will contribute to optimum decision making, data movement, as well as better resource allocation and energy management, also leading to enhanced Quality of Service. The research presented in this paper is work in progress carried out in the context of the VISION Cloud project.
894	The paper discusses a resource management method at the cloud tenant level. It concerns efficiently running a profitable service on resources leased from a cloud infrastructure provider. Particularly, the paper focuses on services that handle parallelizable client requests, e.g, such a request can be processed using a MPI or a MapReduce program. The objective is to manage resource use to just satisfy the performance requirements of clients and avoid the common over-provisioning or under-provisioning problem. The proposed resource management method makes initial resource leasing plan based on client request profile and performance targets in service level agreements (SLAs). It then dynamically adjusts resource allocation based on monitoring data. Our extensive experiments show that the method is able to provision and efficiently use resources to just satisfy the SLA targets.
900	When running mission-critical web-facing applications (e.g., electronic commerce) in cloud environments, predictable response time, e.g., specified as service level agreements (SLA), is a major performance reliability requirement. Through extensive measurements of n-tier application benchmarks in a cloud environment, we study three factors that significantly impact the application response time predictability: bursty workloads (typical of web-facing applications), soft resource management strategies (e.g., global thread pool or local thread pool), and bursts in system software consumption of hardware resources (e.g., Java Virtual Machine garbage collection). Using a set of profit-based performance criteria derived from typical SLAs, we show that response time reliability is brittle, with large response time variations (order of several seconds) depending on each one of those factors. For example, for the same workload and hardware platform, modest increases in workload burstiness may result in profit drops of more than 50\%. Our results show that profitbased performance criteria may contribute significantly to the successful delimitation of performance unreliability boundaries and thus support effective management of clouds.
902	Cloud computing is an emerging paradigm that allows the on-demand delivering of software, hardware, and data as services. It has attracted a lot of attention recently due to the increasing demand for high performance computing and storage. Resource allocation is one of the most important challenges in the cloud computing system, especially when the clients have some Service Level Agreements (SLAs) and the total profit depends on how the system can meet these SLAs. A set of multiple cloud service providers (CSPs) in the cloud, such as Google or Amazon, may support the similar type of application, and therefore, service requests generated from the network edges are free to be dispatched to any CSP in the set. This paper considers the problem of SLA-based resource provisioning and management among different CSPs. Each CSP owns a set of potentially heterogeneous servers supporting a common application type, and each performs resource allocation in these servers for request processing. In the cloud, a central request dispatcher allocates service requests to different servers (belonging to potentially different CSPs) based on the amounts of allocated resources in those servers. Each CSP optimizes its own profit, which is the total revenue obtained from servicing the clients subtracted by the total energy cost. The total revenue depends on the average service request response time as specified in the SLAs. The resource allocation problem among multiple CSPs forms a competitive normal-form game, since the payoff (profit) of each CSP depends not only on its own resource allocation results but also on the actions of the other CSPs. The existence and uniqueness of Nash equilibrium in this game are proved. Each CSP will find its optimal strategy at the Nash equilibrium point using the convex optimization technique. Experimental results demonstrate the effectiveness of the game theoretic resource provisioning framework for the CSPs.
905	
908	Cloud computing customers currently host all of their application components at a single cloud provider. Single-provider hosting eases maintenance tasks, but reduces resilience to failures. Recent research (Li et al., 2010) also shows that providers� offers differ greatly in performance and price, and no single provider is the best in all service categories. In this paper we investigate the benefits of allocating components of a distributed application on multiple public clouds (multi-cloud). We propose a resource allocation algorithm that minimizes the overall cloud operation cost, while satisfying required service-level agreements (SLAs). In spite of the additional delays for inter-cloud communication and the additional costs for inter-cloud data transfer, our simulation study, using real cloud performance and cost data, demonstrates that multi-cloud allocation outperforms single-cloud allocations in a variety of realistic scenarios.
911	Cloud computing has been considered as a solution for solving enterprise application distribution and configuration challenges in the traditional software sales model. Migrating from traditional software to Cloud enables on-going revenue for software providers. However, in order to deliver hosted services to customers, SaaS companies have to either maintain their own hardware or rent it from infrastructure providers. This requirement means that SaaS providers will incur extra costs. In order to minimize the cost of resources, it is also important to satisfy a minimum service level to customers. Therefore, this paper proposes resource allocation algorithms for SaaS providers who want to minimize infrastructure cost and SLA violations. Our proposed algorithms are designed in a way to ensure that Saas providers are able to manage the dynamic change of customers, mapping customer requests to infrastructure level parameters and handling heterogeneity of Virtual Machines. We take into account the customers' Quality of Service parameters such as response time, and infrastructure level parameters such as service initiation time. This paper also presents an extensive evaluation study to analyze and demonstrate that our proposed algorithms minimize the SaaS provider's cost and the number of SLA violations in a dynamic resource sharing Cloud environment.
913	Cloud computing is a solution for addressing challenges such as licensing, distribution, configuration, and operation of enterprise applications associated with the traditional IT infrastructure, software sales and deployment models. Migrating from a traditional model to the Cloud model reduces the maintenance complexity and cost for enterprise customers, and provides on-going revenue for Software as a Service (SaaS) providers. Clients and SaaS providers need to establish a Service Level Agreement (SLA) to define the Quality of Service (QoS). The main objectives of SaaS providers are to minimize cost and to improve Customer Satisfaction Level (CSL). In this paper, we propose customer driven SLA-based resource provisioning algorithms to minimize cost by minimizing resource and penalty cost and improve CSL by minimizing SLA violations. The proposed provisioning algorithms consider customer profiles and providers' quality parameters (e.g., response time) to handle dynamic customer requests and infrastructure level heterogeneity for enterprise systems. We also take into account customer-side parameters (such as the proportion of upgrade requests), and infrastructure-level parameters (such as the service initiation time) to compare algorithms. Simulation results show that our algorithms reduce the total cost up to 54 percent and the number of SLA violations up to 45 percent, compared with the previously proposed best algorithm.
917	In Cloud computing, users with different service requirements often need to negotiate with service provider via Service Level Agreement (SLA). The unique pay-as-you-go billing way in Cloud computing challenges resource provisioning for service providers. In this paper, based on the Dirichlet multinomial model, we present an efficient reputation-based QoS provisioning scheme, which can minimize the cost of computing resources, while satisfying the desired QoS metrics. Unlike the previous counterparts, we consider the statistical probability of the response time as a practical metric rather than the typical mean response time. Numerical results show the efficiency and effectiveness of the proposed scheme.
920	In a cloud computing environment, resources are shared among different clients. Intelligently managing and allocating resources among various clients is important for system providers, whose business model relies on managing the infrastructure resources in a cost-effective manner while satisfying the client service level agreements (SLAs). In this paper, we address the issue of how to intelligently manage the resources in a shared cloud database system and present SmartSLA, a cost-aware resource management system. SmartSLA consists of two main components: the system modeling module and the resource allocation decision module. The system modeling module uses machine learning techniques to learn a model that describes the potential profit margins for each client under different resource allocations. Based on the learned model, the resource allocation decision module dynamically adjusts the resource allocations in order to achieve the optimum profits. We evaluate SmartSLA by using the TPC-W benchmark with workload characteristics derived from real-life systems. The performance results indicate that SmartSLA can successfully compute predictive models under different hardware resource allocations, such as CPU and memory, as well as database specific resources, such as the number of replicas in the database systems. The experimental results also show that SmartSLA can provide intelligent service differentiation according to factors such as variable workloads, SLA levels, resource costs, and deliver improved profit margins.
934	Cloud federation can allow individual Cloud providers working collaboratively to offer best-effort services to service customers. However, the current federated Cloud computing model is not appropriate for computationally intensive Real-time Online Interactive Applications (ROIA). This paper discusses how we propose and develop a business-oriented federated Cloud computing model where multiple independent infrastructure providers can cooperate seamlessly to provide scalable IT infrastructure and QoS-assured hosting services for ROIA. The distinct features of this proposed Cloud federation model is its business layer that can provide an enhanced security features and can trigger the on-demand resource provisioning across multiple infrastructure providers, hence helping to maximize the customer satisfaction, business benefits and resources usage.
941	"Virtualized resource renting is a key issue in IaaS (Infrastructure-as-a-Service) cloud systems. Suitable resource allocation improves resource utilization and increases profit for application providers. The application provider will obtain better revenues according to the Service Level Agreement (SLA), if they rent more virtual resources. However, they will invest much more capital for renting these virtual resources. How many resources a provider rents has become a key thing for cloud applications. This paper addresses the reconciliation objectives by proposing a profit-driven resource scheduling method for virtualized cloud systems. Compared with traditional methods, our method aims at maximizing the revenues by introducing SLA and the cost of renting cloud resource, instead of increasing resource utilization or decreasing early finishing time. We model the performance of applications with queueing theory
943	Cloud Computing provides a convenient means of remote and pay-per-use access to computing resources in forms of Virtual Machines (VMs). Specially, with cloud computing, service providers no longer need to maintain a large number of expensive physical machines, which can significantly reduce the cost. However, due to the fluctuation and uncertainty of the future demands, it is still a challenge for service providers to dynamically determine the optimal resource provisioning to save cost while guaranteeing the Service Level Agreement (SLA). Overload may result in the service unavailable for the latter service requests while over-provisioning naturally increases the cost. To address the problem, in this paper, by defining the unavailability probability of the service as a metric of SLA, we propose a SLA-driven dynamic VMs provisioning strategy based on the large deviation principle, which is capable of proactive calculating the optimal number of VMs for the upcoming demands subject to the unavailability probability below a desired threshold. Finally, the experiments are performed based on real workload traces to show the attainable performance of the proposed resource provisioning strategy and verify that the proposed strategy can make a good tradeoff between saving cost and guaranteeing SLA.
946	Database-as-a-Service (DBaaS) has gain significant momentum with the prevailing usage of Cloud computing. Multi-tenancy is one of the key features of DBaaS offering, where a large volume of databases with different Service Level Agreement (SLA) requirements are co-located in one environment and sharing resources. As Cloud resources are elastic and resource demands of database requests are unpredictable, it is challenging to decide when and where to place databases in Cloud environment according to their resource requirements. In this paper, we propose a cost-efficient placement algorithm striving to produce placement solution that optimizes multiple objectives considering multi-resource constraints, user preferences and system preferences. The objective is to help DBaaS providers to achieve effective resource allocation among multiple databases, minimize the disturbance to the system caused by database migration, and maximize Cloud resource utilization. The demonstrated online placement technique can be used as decision making reference for DBaaS providers to make optimal resource planning. The effectiveness and efficiency of the algorithm have been verified by intensive simulation experiments and real-case study in IBM cloud platform.
947	Virtualized resource allocation for multi-tier web applications in cloud environment brings new challenges to cloud infrastructure providers. In order to meet the constraint of SLA and allocate the available virtualized resources optimally, this paper proposes a resource allocation algorithm for infrastructure providers who want to minimize infrastructure cost and SLA violations. Our proposed algorithm can maximize the overall profit of cloud infrastructure providers when SLA guarantees are satisfied or violated in a dynamic resource sharing cloud environment. The experimental evaluation with a EUCALYPTUS-based cloud and a realistic workload, and the comparison with the existing algorithm demonstrate the feasibility of the algorithm and allow a cost effective usage of resources in cloud simulation environment.
948	Software as a Service (SaaS) in Cloud is getting more and more significant among software users and providers recently. A SaaS that is delivered as composite application has many benefits including reduced delivery costs, flexible offers of the SaaS functions and decreased subscription cost for users. However, this approach has introduced a new problem in managing the resources allocated to the composite SaaS. The resource allocation that has been done at the initial stage may be overloaded or wasted due to the dynamic environment of a Cloud. A typical data center resource management usually triggers a placement reconfiguration for the SaaS in order to maintain its performance as well as to minimize the resource used. Existing approaches for this problem often ignore the underlying dependencies between SaaS components. In addition, the reconfiguration also has to comply with SaaS constraints in terms of its resource requirements, placement requirement as well as its SLA. To tackle the problem, this paper proposes a penalty-based Grouping Genetic Algorithm for multiple composite SaaS components clustering in Cloud. The main objective is to minimize the resource used by the SaaS by clustering its component without violating any constraint. Experimental results demonstrate the feasibility and the scalability of the proposed algorithm.
952	As an emerging technique, cloud computing has been suffering challenges from its quality of service (QoS) guarantee. Any violation of SLA would lead to a loss for both cloud providers and customers. To solve this problem, dynamical management of resources to ensure the QoS has been put forward as a demanding work in the cloud. In this paper, we present an adaptive QoS-aware cloud based on the Eucalyptus, which can dynamically scale up and down to guarantee QoS negotiated via SLA. Our adaptive QoS-aware cloud employs two load balancing policies and monitors the application-level QoS to scale adaptively. We conduct the evaluation in open source Eucalyptus based on XEN virtual machine monitor (VMM). As demonstrated in our experiment, our solution presents an adaptive scalability while maintaining the QoS.
961	Power is becoming an increasingly important concern for large-scale cloud computing systems. Meanwhile, cloud service providers leverage virtualization technologies to facilitate service consolidation and enhance resource utilization. However, the introduction of virtualization makes the cloud infrastructure more complex, and thus challenges cloud power management. In a virtualized environment, resource needs to be configured at runtime at the cloud, server and virtual machine levels to achieve high power efficiency. In addition, cloud power management should guarantee high users' SLA (service level agreement) satisfaction. In this paper, we present an adaptive power management framework in the cloud to achieve autonomic resource configuration. We propose a software and lightweight approach to accurately estimate the power usage of virtual machines and cloud servers. It explores hypervisor-observable performance metrics to build the power usage model. To configure cloud resources, we consider both the system power usage and the SLA requirements, and leverage learning techniques to achieve autonomic resource allocation and optimal power efficiency. We implement a prototype of the proposed power management system and test it on a cloud testbed. Experimental results show the high accuracy (over 90\%) of our power usage estimation mechanism and our resource configuration approach achieves the lowest energy usage among the compared four approaches.
963	Recently, cloud computing has emerged as a new computing paradigm on the Internet. With the development of cloud computing, enterprise data centers shift towards a utility computing model where many critical business applications share a common pool of infrastructure resources offering capacity on demand. The virtual machine with the features of strong isolation and flexible is usually assigned as the basic unit. However, as the demand of each type of VM can fluctuate independently at run time, it becomes a challenging problem to allocate data center resources to each VM to balance the workload in the cloud. In this paper, we introduce an approach (Statistic based Load Balance, SLB) that makes use of the statistical prediction and available resource evaluation mechanism to make online resource allocation decisions. Unlike the methods that balance load based on SLA (Service Level Agreement) of VMs, SLB achieves load balancing by predicting the VM's resource demand. The approach includes two parts:(1) A data analysis of on-line historical performance for forecasting the resource demand of each VM, and (2) An algorithm for choosing a proper host in the resource pool to run the VM. Experiments show that SLB can perform load balance in time, and also perform more balanced use of different resources.
970	Increasingly video-on-demand (VoD) applications have been ported to cloud platforms. Leveraging the elastic resource provisioning of the cloud, it is believed that VoD applications should attain high performance cost-effectively. In this paper we propose an approach that aims to solve the fundamental resource reservation and scheduling problem of configuring the cloud utility to meet SLAs for VoD applications at a modest cost. First, we devise a constraint-based model that describes the relationship among channel placement, user groups' bandwidth allocation, operating costs and QoS constraints. Second, we present a distributed heuristic algorithm, called DREAM, that solves the model and produces a budget solution that reserves and allocates cloud bandwidth, and determines the channel layout among datacenters. Simulations driven by data traces collected from a commercial VoD system demonstrate that DREAM provides much better access locality and data availability than and comparable streaming quality to state-of-the-art solutions at lower cloud operating costs.
1026	"With the advent of 4G and other long-term evolution (LTE) wireless networks, the traditional boundaries of patient record propagation are diminishing as networking technologies extend the reach of hospital infrastructure and provide on-demand mobile access to medical multimedia data. However, due to legacy and proprietary software, storage and decommissioning costs, and the price of centralization and redevelopment, it remains complex, expensive, and often unfeasible for hospitals to deploy their infrastructure for online and mobile use. This paper proposes the SparkMed data integration framework for mobile healthcare (m-Health), which significantly benefits from the enhanced network capabilities of LTE wireless technologies, by enabling a wide range of heterogeneous medical software and database systems (such as the picture archiving and communication systems, hospital information system, and reporting systems) to be dynamically integrated into a cloud-like peer-to-peer multimedia data store. Our framework allows medical data applications to share data with mobile hosts over a wireless network (such as WiFi and 3G), by binding to existing software systems and deploying them as m-Health applications. SparkMed integrates techniques from multimedia streaming, rich Internet applications (RIA), and remote procedure call (RPC) frameworks to construct a Self-managing, Pervasive Automated netwoRK for Medical Enterprise Data (SparkMed). Further, it is resilient to failure, and able to use mobile and handheld devices to maintain its network, even in the absence of dedicated server devices. We have developed a prototype of the SparkMed framework for evaluation on a radiological workflow simulation, which uses SparkMed to deploy a radiological image viewer as an m-Health application for telemedical use by radiologists and stakeholders. We have evaluated our prototype using ten devices over WiFi and 3G, verifying that our framework meets its two main objectives: 1) interactive delivery of medical multimedia data to mobile devices
1113	Today in the world of cloud and grid computing integration of data from heterogeneous databases is inevitable. Virtual Database Technology (VDB) is one of the effective solutions for integration of data from heterogeneous sources. This will become complex when size of the database is very large. MapReduce is a new framework specifically designed for processing huge datasets on distributed sources. Apache's Hadoop is an implementation of MapReduce. Currently Hadoop has been applied successfully for file based datasets. This paper proposes to utilize the parallel and distributed processing capability of Hadoop MapReduce for handling heterogeneous query execution on large datasets. So Virtual Database Engine built on top of this will result in effective high performance distributed data integration.
1125	vLEARM � the Virtual Laboratory for Environmental Accounting and Resource Management � is a high-level system design for environmental accounting in Australia. vLEARM provides for the use of multiple data layers � including those that are not image-based � held in the Source Data Layer via integrative functionality in the Common Raster Data Layer. The Business Logic Layer converts raster data into user-driven environmental information through SQL queries, combinatorial algorithms, and temporal landscape models. Finally, the Presentation and Federation Layer enhances and broadens use by managing functionality across multiple platforms and/or within the Cloud, and providing a �workbench� interface that enables custom workflows to be implemented without high-level IT expertise. It is estimated that the development of a fully functional system based on the vLEARM concept will require five to 10 years. Once produced, such a system will significantly contribute realising the full value of past and future investments in digital imagery.
1126	The advent of Cloud Computing introduced new challenges in various computer science fields and disciplines, monitoring being one of them. Due to the multi-tenant nature of Cloud environment, its size and use of massive virtualization, the current monitoring solutions are approaching their limits. Unlike some other approaches, focusing on data integration, aggregation, and abstraction, our work focuses on a virtual machine representing the primary source of monitoring information. In this paper, we propose requirements for the producer of monitoring information addressing existing issues related to monitoring data representation, storage, processing and distribution. As a proof-of-concept, conforming to these requirements, prototype of event-based monitoring daemon is presented in detail. The resulting solution allows multiple users to consume monitoring information in an extensible data format without impairing interoperability.
1135	"Cloud Computing is often described as ""resources accessed via a browser over the Internet."" However, this definition has become increasingly insufficient to characterize the breadth of applications and use cases for the cloud, and the networks that must support them. A broadening range of endpoints are accessing the cloud: browser-free device apps, multimedia endpoints such as video and game consoles, sensor networks, servers, and storage. The wireline and wireless network requirements-e.g., jitter, latency, packet loss, protocol support-for these uses vary, and imply that a variety of network capabilities are sometimes necessary: e.g., MPLS for quality of service via class of service to support interactive high definition video in the cloud
1312	"Resource allocation and call admission control (CAC) are key management functions in future cellular networks, in order to provide multimedia applications to mobiles users with quality of service (QoS) guarantees and efficient resource utilization. In this paper, we propose and analyze a priority based resource sharing scheme for voice/data integrated cellular networks. The unique features of the proposed scheme are that 1) the maximum resource utilization can be achieved, since all the leftover capacity after serving the high priority voice traffic can be utilized by the data traffic
1319	Sensor networks serve as a natural data source to data stream management systems (DSMSs), and in return DSMSs are capable of executing much more complex operations on the data than nodes in the network, allowing a wider variety of queries to be performed on sensor produced data. Integration of these systems to create a unified query plan that is executed across DSMS/sensor boundaries is not a trivial task because of the different architectures and assumptions of these systems. This chapter presents an integrated query processing environment where users can seamlessly query both a data stream management system and a sensor network with one query expression. By integrating the two query processing systems, the optimization goals of the sensor network (primarily power) and server network (primarily latency and quality) can be unified into one quality of service metric. The chapter shows various steps of the unified optimization process for a sample query where the effects of each step that the optimizer takes can be directly viewed using a quality of service monitor. The chapter includes sensors deployed in the demo area in a tiny mockup of a factory application.
1329	A tremendous amount of investigations and realisations has been carried on for the last ten years in the domains of communication media and protocols, distributed database management, distributed operating systems and reliability resulting in significant progresses towards standardization. However, most of these results apply to on-line systems and relatively very little of this work is related to hard-real-time environments whereby specified bounds on processing delays are stringent and ranging from few millisecunds to few hundreds of millisecunds. Based on the ISO OSI-Reference Model analysis, rationale are presented for a reliable connectionless-mode, message due-date-dependant qualities of services and flexible conversation protocols above the transport layer. So as to support time-dependant transmission protocols, it is shown that the IEEE 802 Local Area Network standards must be enhanced in order to provide message transmission scheduling based on message due-dates. A task priority is defined as a function of both its static priority and its due-date. Data access serialization induces precedence constraints on task scheduling. Task resource allocation conflicts are solved though a unique task-priority-dependant resolution rule.
1347	"Cloud Computing is often described as ""resources accessed via a browser over the Internet."" However, this definition has become increasingly insufficient to characterize the breadth of applications and use cases for the cloud, and the networks that must support them. A broadening range of endpoints are accessing the cloud: browser-free device apps, multimedia endpoints such as video and game consoles, sensor networks, servers, and storage. The wireline and wireless network requirements-e.g., jitter, latency, packet loss, protocol support-for these uses vary, and imply that a variety of network capabilities are sometimes necessary: e.g., MPLS for quality of service via class of service to support interactive high definition video in the cloud
